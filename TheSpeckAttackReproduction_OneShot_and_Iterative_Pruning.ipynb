{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TheSpeckAttackReproduction_OneShot_and_Iterative_Pruning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ8MxqYz3GsA"
      },
      "source": [
        "!pip install lottery-ticket-pruner\n",
        "!pip install matplotlib\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3g4Wgl63dgP"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from lottery_ticket_pruner import LotteryTicketPruner\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Fw-MRgjau1"
      },
      "source": [
        "# The Speck cipher and data generation algorithms #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/speck.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj5GsxGJ3hlH"
      },
      "source": [
        "import numpy as np\n",
        "from os import urandom\n",
        "\n",
        "def WORD_SIZE():\n",
        "    return(16);\n",
        "\n",
        "def ALPHA():\n",
        "    return(7);\n",
        "\n",
        "def BETA():\n",
        "    return(2);\n",
        "\n",
        "MASK_VAL = 2 ** WORD_SIZE() - 1;\n",
        "\n",
        "def shuffle_together(l):\n",
        "    state = np.random.get_state();\n",
        "    for x in l:\n",
        "        np.random.set_state(state);\n",
        "        np.random.shuffle(x);\n",
        "\n",
        "def rol(x,k):\n",
        "    return(((x << k) & MASK_VAL) | (x >> (WORD_SIZE() - k)));\n",
        "\n",
        "def ror(x,k):\n",
        "    return((x >> k) | ((x << (WORD_SIZE() - k)) & MASK_VAL));\n",
        "\n",
        "def enc_one_round(p, k):\n",
        "    c0, c1 = p[0], p[1];\n",
        "    c0 = ror(c0, ALPHA());\n",
        "    c0 = (c0 + c1) & MASK_VAL;\n",
        "    c0 = c0 ^ k;\n",
        "    c1 = rol(c1, BETA());\n",
        "    c1 = c1 ^ c0;\n",
        "    return(c0,c1);\n",
        "\n",
        "def dec_one_round(c,k):\n",
        "    c0, c1 = c[0], c[1];\n",
        "    c1 = c1 ^ c0;\n",
        "    c1 = ror(c1, BETA());\n",
        "    c0 = c0 ^ k;\n",
        "    c0 = (c0 - c1) & MASK_VAL;\n",
        "    c0 = rol(c0, ALPHA());\n",
        "    return(c0, c1);\n",
        "\n",
        "def expand_key(k, t):\n",
        "    ks = [0 for i in range(t)];\n",
        "    ks[0] = k[len(k)-1];\n",
        "    l = list(reversed(k[:len(k)-1]));\n",
        "    for i in range(t-1):\n",
        "        l[i%3], ks[i+1] = enc_one_round((l[i%3], ks[i]), i);\n",
        "    return(ks);\n",
        "\n",
        "def encrypt(p, ks):\n",
        "    x, y = p[0], p[1];\n",
        "    for k in ks:\n",
        "        x,y = enc_one_round((x,y), k);\n",
        "    return(x, y);\n",
        "\n",
        "def decrypt(c, ks):\n",
        "    x, y = c[0], c[1];\n",
        "    for k in reversed(ks):\n",
        "        x, y = dec_one_round((x,y), k);\n",
        "    return(x,y);\n",
        "\n",
        "def check_testvector():\n",
        "  key = (0x1918,0x1110,0x0908,0x0100)\n",
        "  pt = (0x6574, 0x694c)\n",
        "  ks = expand_key(key, 22)\n",
        "  ct = encrypt(pt, ks)\n",
        "  if (ct == (0xa868, 0x42f2)):\n",
        "    print(\"Testvector verified.\")\n",
        "    return(True);\n",
        "  else:\n",
        "    print(\"Testvector not verified.\")\n",
        "    return(False);\n",
        "\n",
        "#convert_to_binary takes as input an array of ciphertext pairs\n",
        "#where the first row of the array contains the lefthand side of the ciphertexts,\n",
        "#the second row contains the righthand side of the ciphertexts,\n",
        "#the third row contains the lefthand side of the second ciphertexts,\n",
        "#and so on\n",
        "#it returns an array of bit vectors containing the same data\n",
        "def convert_to_binary(arr):\n",
        "  X = np.zeros((4 * WORD_SIZE(),len(arr[0])),dtype=np.uint8);\n",
        "  for i in range(4 * WORD_SIZE()):\n",
        "    index = i // WORD_SIZE();\n",
        "    offset = WORD_SIZE() - (i % WORD_SIZE()) - 1;\n",
        "    X[i] = (arr[index] >> offset) & 1;\n",
        "  X = X.transpose();\n",
        "  return(X);\n",
        "\n",
        "#takes a text file that contains encrypted block0, block1, true diff prob, real or random\n",
        "#data samples are line separated, the above items whitespace-separated\n",
        "#returns train data, ground truth, optimal ddt prediction\n",
        "def readcsv(datei):\n",
        "    data = np.genfromtxt(datei, delimiter=' ', converters={x: lambda s: int(s,16) for x in range(2)});\n",
        "    X0 = [data[i][0] for i in range(len(data))];\n",
        "    X1 = [data[i][1] for i in range(len(data))];\n",
        "    Y = [data[i][3] for i in range(len(data))];\n",
        "    Z = [data[i][2] for i in range(len(data))];\n",
        "    ct0a = [X0[i] >> 16 for i in range(len(data))];\n",
        "    ct1a = [X0[i] & MASK_VAL for i in range(len(data))];\n",
        "    ct0b = [X1[i] >> 16 for i in range(len(data))];\n",
        "    ct1b = [X1[i] & MASK_VAL for i in range(len(data))];\n",
        "    ct0a = np.array(ct0a, dtype=np.uint16); ct1a = np.array(ct1a,dtype=np.uint16);\n",
        "    ct0b = np.array(ct0b, dtype=np.uint16); ct1b = np.array(ct1b, dtype=np.uint16);\n",
        "    \n",
        "    #X = [[X0[i] >> 16, X0[i] & 0xffff, X1[i] >> 16, X1[i] & 0xffff] for i in range(len(data))];\n",
        "    X = convert_to_binary([ct0a, ct1a, ct0b, ct1b]); \n",
        "    Y = np.array(Y, dtype=np.uint8); Z = np.array(Z);\n",
        "    return(X,Y,Z);\n",
        "\n",
        "#baseline training data generator\n",
        "def make_train_data(n, nr, diff=(0x0040,0)):\n",
        "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
        "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
        "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
        "  num_rand_samples = np.sum(Y==0);\n",
        "  plain1l[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  plain1r[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  ks = expand_key(keys, nr);\n",
        "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
        "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
        "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
        "  return(X,Y);\n",
        "\n",
        "#real differences data generator\n",
        "def real_differences_data(n, nr, diff=(0x0040,0)):\n",
        "  #generate labels\n",
        "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
        "  #generate keys\n",
        "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
        "  #generate plaintexts\n",
        "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  #apply input difference\n",
        "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
        "  num_rand_samples = np.sum(Y==0);\n",
        "  #expand keys and encrypt\n",
        "  ks = expand_key(keys, nr);\n",
        "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
        "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
        "  #generate blinding values\n",
        "  k0 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  k1 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  #apply blinding to the samples labelled as random\n",
        "  ctdata0l[Y==0] = ctdata0l[Y==0] ^ k0; ctdata0r[Y==0] = ctdata0r[Y==0] ^ k1;\n",
        "  ctdata1l[Y==0] = ctdata1l[Y==0] ^ k0; ctdata1r[Y==0] = ctdata1r[Y==0] ^ k1;\n",
        "  #convert to input data for neural networks\n",
        "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
        "  return(X,Y);"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dwkFadkY_l"
      },
      "source": [
        "# Create folders for saving models for later use if needed #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYR7jt3YhaIs"
      },
      "source": [
        "wdir_curent_bw = \"./current_bw/\";\n",
        "wdir_curent_bw_all= \"./current_bw_all/\";\n",
        "init_trained= \"./trained/\";\n",
        "init_trained_all= \"./trained_all/\";\n",
        "\n",
        "os.mkdir(wdir_curent_bw);\n",
        "os.mkdir(wdir_curent_bw_all);\n",
        "os.mkdir(init_trained);\n",
        "os.mkdir(init_trained_all);\n",
        "\n",
        "os.mkdir(\"./best\");\n",
        "os.mkdir(\"./best_all\");\n",
        "os.mkdir(\"./worst\");\n",
        "os.mkdir(\"./worst_all\");"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbC0HjUCkq3q"
      },
      "source": [
        "# The depth-1/10 distinguisher implementation#\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/train_nets.py and slightly adapted for running multiple trials. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMz2UrOU3mi8"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Conv1D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, Concatenate,Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.regularizers import l2, l1, l1_l2\n",
        "\n",
        "\n",
        "def cyclic_lr(num_epochs, high_lr, low_lr):\n",
        "  res = lambda i: low_lr + ((num_epochs-1) - i % num_epochs)/(num_epochs-1) * (high_lr - low_lr);\n",
        "  return(res);\n",
        "\n",
        "def make_checkpoint_best_worst(datei):\n",
        "  res = ModelCheckpoint(datei, monitor='val_acc', save_best_only = True, save_weights_only=True );\n",
        "  return(res);\n",
        "\n",
        "def make_checkpoint_all(datei):\n",
        "  res = ModelCheckpoint(datei, monitor='val_acc', save_best_only = False, save_weights_only=True);\n",
        "  return(res);\n",
        "\n",
        "\n",
        "wdir_curent_bw = \"./current_bw/\";\n",
        "wdir_curent_bw_all= \"./current_bw_all/\";\n",
        "\n",
        "bs = 5000;\n",
        "\n",
        "def make_resnet(num_blocks=2, num_filters=32, num_outputs=1, d1=64, d2=64, word_size=16, ks=3,depth=5, reg_param=0.0001, final_activation='sigmoid'):\n",
        "  #Input and preprocessing layers\n",
        "  inp = Input(shape=(num_blocks * word_size * 2,));\n",
        "  rs = Reshape((2 * num_blocks, word_size))(inp);\n",
        "  perm = Permute((2,1))(rs);\n",
        "  #add a single residual layer that will expand the data to num_filters channels\n",
        "  #this is a bit-sliced layer\n",
        "  conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm);\n",
        "  conv0 = BatchNormalization()(conv0);\n",
        "  conv0 = Activation('relu')(conv0);\n",
        "  #add residual blocks\n",
        "  shortcut = conv0;\n",
        "  for i in range(depth):\n",
        "    conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut);\n",
        "    conv1 = BatchNormalization()(conv1);\n",
        "    conv1 = Activation('relu')(conv1);\n",
        "    conv2 = Conv1D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1);\n",
        "    conv2 = BatchNormalization()(conv2);\n",
        "    conv2 = Activation('relu')(conv2);\n",
        "    shortcut = Add()([shortcut, conv2]);\n",
        "  #add prediction head\n",
        "  flat1 = Flatten()(shortcut);\n",
        "  dense1 = Dense(d1,kernel_regularizer=l2(reg_param))(flat1);\n",
        "  dense1 = BatchNormalization()(dense1);\n",
        "  dense1 = Activation('relu')(dense1);\n",
        "  dense2 = Dense(d2, kernel_regularizer=l2(reg_param))(dense1);\n",
        "  dense2 = BatchNormalization()(dense2);\n",
        "  dense2 = Activation('relu')(dense2);\n",
        "  out = Dense(num_outputs, activation=final_activation, kernel_regularizer=l2(reg_param))(dense2);\n",
        "  model = Model(inputs=inp, outputs=out);\n",
        "\n",
        "  return model\n",
        "\n",
        "def model_builder(depth):\n",
        "\n",
        "  model = make_resnet(depth=depth);\n",
        "  model.compile(\n",
        "          optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc']);\n",
        "  return model;\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def train_speck_distinguisher(model, num_epochs, num_rounds,rep_trial,wdirbw, wdirall, pruning_method,pruning_trial, X_train, Y_train, X_eval, Y_eval):\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights= True);\n",
        "    check_bw = make_checkpoint_best_worst(wdirbw +str(rep_trial)+ pruning_method +str(pruning_trial)+\"{epoch:02d}.hdf5\");\n",
        "    check_bw_all = make_checkpoint_all(wdirall+str(rep_trial) + pruning_method +str(pruning_trial)+\"{epoch:02d}.hdf5\");\n",
        "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
        "    history = model.fit(X_train, Y_train, batch_size= bs, epochs=num_epochs,validation_data=(X_eval, Y_eval), callbacks=[lr, stop_early, check_bw, check_bw_all])\n",
        "    best_epoch = np.argmax(history.history['val_acc']) + 1\n",
        "    return(model,best_epoch);\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q4pslswlFMo"
      },
      "source": [
        "Rename folders that keep the best/worst models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uFJZtjDSWRU"
      },
      "source": [
        "def rename_best():\n",
        "  shutil.rmtree(\"./best\");\n",
        "  shutil.rmtree(\"./best_all\");\n",
        "\n",
        "  os.rename(wdir_curent_bw, \"./best\");\n",
        "  os.rename(wdir_curent_bw_all, \"./best_all\");\n",
        "    \n",
        "  os.mkdir(wdir_curent_bw);\n",
        "  os.mkdir(wdir_curent_bw_all);\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0S-cstJTRmf"
      },
      "source": [
        "def rename_worst():\n",
        "  shutil.rmtree(\"./worst\");\n",
        "  shutil.rmtree(\"./worst_all\");\n",
        "\n",
        "  os.rename(wdir_curent_bw, \"./worst\");\n",
        "  os.rename(wdir_curent_bw_all, \"./worst_all\");\n",
        "    \n",
        "  os.mkdir(wdir_curent_bw);\n",
        "  os.mkdir(wdir_curent_bw_all);\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd7xyudBlX_6"
      },
      "source": [
        "For saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKSyxCZChaIw"
      },
      "source": [
        "def save_model(model, name):\n",
        "  model_save_name = name;\n",
        "  #path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "  model.save(name)\n",
        "  print(\"\\n\");"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2qmQ_i1liKn"
      },
      "source": [
        "# Evaluate the results #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/eval.py and slightly adapted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Iifwm54h6a"
      },
      "source": [
        "def evaluate(net,X,Y):\n",
        "    Z = net.predict(X,batch_size=10000).flatten();\n",
        "    Zbin = (Z > 0.5);\n",
        "    diff = Y - Z; \n",
        "    mse = np.mean(diff*diff);\n",
        "    n = len(Z); n0 = np.sum(Y==0); n1 = np.sum(Y==1);\n",
        "    acc = np.sum(Zbin == Y) / n;\n",
        "    tpr = np.sum(Zbin[Y==1]) / n1;\n",
        "    tnr = np.sum(Zbin[Y==0] == 0) / n0;\n",
        "    mreal = np.median(Z[Y==1]);\n",
        "    high_random = np.sum(Z[Y==0] > mreal) / n0;\n",
        "\n",
        "    return(acc, tpr, tnr); # added\n",
        "    #print(\"Accuracy: \", acc, \"TPR: \", tpr, \"TNR: \", tnr, \"MSE:\", mse);\n",
        "    #print(\"Percentage of random pairs with score higher than median of real pairs:\", 100*high_random);"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NePJpLj0l3il"
      },
      "source": [
        "# Conducting multiple evaluations #\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDVHt8dGFCHu"
      },
      "source": [
        "def multiple_evaluations(model, repetitions, num_rounds):\n",
        " \n",
        "  accs = [];\n",
        "  tprs = [];\n",
        "  tnrs = [];\n",
        "  for i in range(0, repetitions):\n",
        "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "\n",
        "    (acc, tpr, tnr) = evaluate(model, X_eval, Y_eval);\n",
        "    accs.append(acc);\n",
        "    tprs.append(tpr);\n",
        "    tnrs.append(tnr);\n",
        "\n",
        "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
        "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
        "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
        "        \n",
        "  return(np.mean(accs), np.mean(tprs), np.mean(tnrs));\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0dWTS30l-Wy"
      },
      "source": [
        "# Creating a plot with values for accuracy/TPR/TNR for the baseline, pruned with one-shot, and pruned with iterative model #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKmPoVFKM3G7"
      },
      "source": [
        "def create_figure(title, x_label, y_label, x0, y0,xy0err, x1, y1, xy1err, x2, y2, xy2err):\n",
        "    \n",
        "    fig, ax =  plt.subplots(figsize=(10,10));\n",
        "    #trained baseline model \n",
        "    err= ax.errorbar(x0,y0,color = 'k', yerr=xy0err, ecolor='k', elinewidth=1, capsize=3);\n",
        "    err[-1][0].set_linestyle('--'); \n",
        "    #one shot from initial\n",
        "    err=ax.errorbar(x1,y1, color ='y',yerr=xy1err, ecolor=\"y\", elinewidth=1, capsize=3);\n",
        "    err[-1][0].set_linestyle('--');\n",
        "    #iterative from initial\n",
        "    err=ax.errorbar(x2,y2, color='g', yerr=xy2err, ecolor=\"g\", elinewidth=1, capsize=3);\n",
        "    err[-1][0].set_linestyle('--');\n",
        "  \n",
        "    plt.xlabel(x_label);\n",
        "    plt.ylabel(y_label);\n",
        "\n",
        "    plt.savefig(title +\".png\", bbox_inches='tight')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WhkKhrCmBwp"
      },
      "source": [
        "# Pruning a model using the one shot or iterative pruning method for a value of *p%* and training it #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7EPQ-rSxdPC"
      },
      "source": [
        "def apply_pruning_method(max_acc, min_acc,pruning_method, num_repetition, depth, num_rounds, X_train, Y_train, X_eval, Y_eval, num_epochs, num_pruning_trials, initial_baseline_model, initial_baseline_model_weights, trained_baseline_model, trained_baseline_model_weights, prune_strategy, prune_percentage_for_iterative):\n",
        "    \n",
        "    pruner = LotteryTicketPruner(initial_baseline_model);\n",
        "\n",
        "    pruned_accs =[];\n",
        "    pruned_tprs = [];\n",
        "    pruned_tnrs = [];\n",
        "    pruning_trial_no=0;\n",
        "    \n",
        "\n",
        "\n",
        "    for prune_percentage in list(np.linspace(start=0.1,stop=1,num=num_pruning_trials+1))[0:-1]:\n",
        "        pruning_trial_no= pruning_trial_no+1;\n",
        "        if pruning_method==\"one_shot\":\n",
        "          pruner.reset_masks();\n",
        "        elif pruning_method==\"iterative\":\n",
        "          prune_percentage = prune_percentage_for_iterative;\n",
        "          \n",
        "\n",
        "        initial_baseline_model.set_weights(initial_baseline_model_weights);\n",
        "        trained_baseline_model.set_weights(trained_baseline_model_weights);\n",
        "\n",
        "        pruner.calc_prune_mask(trained_baseline_model, prune_percentage, prune_strategy);\n",
        "\n",
        "      \n",
        "        pruner.apply_pruning(initial_baseline_model);\n",
        "\n",
        "\n",
        "\n",
        "        (pruned_trained_model, stoped_epoch) = train_speck_distinguisher(initial_baseline_model,num_epochs, num_rounds, num_repetition, wdir_curent_bw, wdir_curent_bw_all,pruning_method,pruning_trial_no, X_train,  Y_train, X_eval, Y_eval);\n",
        "\n",
        "        print(\"Experiment repetition no:\"+ str(num_repetition)+ \" Pruned model at \" + str(prune_percentage) + \"pruning trial:\"+ str(pruning_trial_no)+\"\\n\");\n",
        "        (acc, tpr, tnr) = multiple_evaluations(pruned_trained_model,10,num_rounds);\n",
        "\n",
        "        if(acc >=max_acc):\n",
        "          max_acc = acc;\n",
        "          rename_best();\n",
        "        if(acc <=min_acc):\n",
        "          min_acc = acc;\n",
        "          rename_worst();\n",
        "\n",
        "        pruned_accs.append(acc);\n",
        "        pruned_tprs.append(tpr);\n",
        "        pruned_tnrs.append(tnr);\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    return(max_acc,min_acc,pruned_accs, pruned_tprs, pruned_tnrs)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12FCecJDmjSL"
      },
      "source": [
        "# Execute the above described pruning for different values of *p%* multiple times#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X3G4E6sVMVT"
      },
      "source": [
        "def execute_experiments(num_rounds=5, num_epochs=40, num_experiment_repetitions=5, num_pruning_trials= 9, depth=10,data_for_experiment=\"Real_vs_Random\",prune_strategy=\"smallest_weights_global\", prune_percentage_for_iterative=0.2):\n",
        "    \n",
        "    max_acc =0;\n",
        "    min_acc =1;\n",
        "    \n",
        "    baseline_accuracies_all_trials=[];\n",
        "    baseline_tprs_all_trials=[];\n",
        "    baseline_tnrs_all_trials=[];\n",
        "\n",
        "    pruned_accuracies_all_trials_one_shot_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "    pruned_tprs_all_trials_one_shot_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "    pruned_tnrs_all_trials_one_shot_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "\n",
        "    pruned_accuracies_all_trials_iterative_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "    pruned_tprs_all_trials_iterative_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "    pruned_tnrs_all_trials_iterative_from_initial=[[] for x in range(num_pruning_trials)];\n",
        "\n",
        "   \n",
        "    \n",
        "    for repetition in range(0,num_experiment_repetitions):\n",
        "\n",
        "        if data_for_experiment==\"Real_vs_Random\":      \n",
        "            X_train, Y_train = make_train_data(10**7,num_rounds);\n",
        "            X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "        elif data_for_experiment==\"Real_differences\":\n",
        "            X_train, Y_train = make_train_data(10**7,num_rounds);\n",
        "            X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "\n",
        "        initial_baseline_model = model_builder(depth);\n",
        "        initial_baseline_model_weights = initial_baseline_model.get_weights();\n",
        "        save_model(initial_baseline_model,\"Initial baseline model \"+str(num_rounds) +\"R\" + \" Experiment trial no:\"+str(repetition));\n",
        "\n",
        "\n",
        "        (trained_baseline_model,trnd_bm_stop_epoch) = train_speck_distinguisher(initial_baseline_model,num_epochs, num_rounds, repetition,init_trained, init_trained_all,\"init_trained\",repetition, X_train, Y_train, X_eval, Y_eval);\n",
        "        trained_baseline_model_weights = trained_baseline_model.get_weights();\n",
        "        save_model(trained_baseline_model,\"Trained baseline model \"+str(num_rounds) +\"R\" + \"Experiment trial no:\"+str(repetition));\n",
        "\n",
        "        print(\"Baseline model results at repetition \"+str(repetition)+\":\\n\");\n",
        "        (b_acc, b_tpr, b_tnr) =multiple_evaluations(trained_baseline_model,10,num_rounds);\n",
        "        baseline_accuracies_all_trials.append(b_acc);\n",
        "        baseline_tprs_all_trials.append(b_tpr);\n",
        "        baseline_tnrs_all_trials.append(b_tnr);\n",
        "\n",
        "        print(\"Starting One-Shot with initialization from original initial network:\\n\");\n",
        "        (max_acc,min_acc,oneShot_from_initial_pruned_accs, oneShot_from_initial_pruned_tprs, oneShot_from_initial_pruned_tnrs)=apply_pruning_method(max_acc,min_acc,\"one_shot\", repetition,depth, num_rounds, X_train, Y_train, X_eval, Y_eval, trnd_bm_stop_epoch, num_pruning_trials, initial_baseline_model, initial_baseline_model_weights, trained_baseline_model, trained_baseline_model_weights, prune_strategy, prune_percentage_for_iterative);\n",
        "        print(\"Starting Iterative with initialization from original initial network:\\n\");\n",
        "        (max_acc, min_acc,iterative_from_initial_pruned_accs, iterative_from_initial_pruned_tprs, iterative_from_initial_pruned_tnrs)=apply_pruning_method(max_acc,min_acc,\"iterative\", repetition, depth, num_rounds, X_train, Y_train, X_eval, Y_eval, trnd_bm_stop_epoch, num_pruning_trials, initial_baseline_model, initial_baseline_model_weights, trained_baseline_model, trained_baseline_model_weights, prune_strategy, prune_percentage_for_iterative);\n",
        "  \n",
        "        for i in range(0,num_pruning_trials):\n",
        "            pruned_accuracies_all_trials_one_shot_from_initial[i].append(oneShot_from_initial_pruned_accs[i]);\n",
        "            pruned_tprs_all_trials_one_shot_from_initial[i].append(oneShot_from_initial_pruned_tprs[i]);\n",
        "            pruned_tnrs_all_trials_one_shot_from_initial[i].append(oneShot_from_initial_pruned_tnrs[i]);\n",
        "\n",
        "            pruned_accuracies_all_trials_iterative_from_initial[i].append(iterative_from_initial_pruned_accs[i]);\n",
        "            pruned_tprs_all_trials_iterative_from_initial[i].append(iterative_from_initial_pruned_tprs[i]);\n",
        "            pruned_tnrs_all_trials_iterative_from_initial[i].append(iterative_from_initial_pruned_tnrs[i]);\n",
        "\n",
        "        \n",
        "    return (    baseline_accuracies_all_trials, baseline_tprs_all_trials, baseline_tnrs_all_trials,\n",
        "                pruned_accuracies_all_trials_one_shot_from_initial, pruned_tprs_all_trials_one_shot_from_initial, pruned_tnrs_all_trials_one_shot_from_initial, \n",
        "                pruned_accuracies_all_trials_iterative_from_initial, pruned_tprs_all_trials_iterative_from_initial, pruned_tnrs_all_trials_iterative_from_initial,\n",
        "           );\n",
        "            "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqz7c8bRnHmy"
      },
      "source": [
        "Compute *p* values for iterative pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlTbq2oYVenT"
      },
      "source": [
        "def compute_pruning_percentages_iterative(num_pruning_trials, pruning_percentage):\n",
        "  remaining_weights=1;\n",
        "  percentages=[];\n",
        "\n",
        "  for i in range(0, num_pruning_trials):\n",
        "    remaining_weights= remaining_weights - remaining_weights*pruning_percentage;\n",
        "    percentages.append(1- remaining_weights);\n",
        "\n",
        "\n",
        "  return (percentages);\n",
        "\n",
        "  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3E0VyE3nSOW"
      },
      "source": [
        "# Create the Accuracy, TPR and TNR plots #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLDsMtOYHQg1"
      },
      "source": [
        "def create_plots_for_executed_experiments(experiment_type, depth, num_rounds, num_experiment_repetitions, num_pruning_trials, prune_percentage_for_iterative, b_accs, b_tprs, b_tnrs, os_init_accs, os_init_tprs, os_init_tnrs, it_init_accs, it_init_tprs, it_init_tnrs):\n",
        "\n",
        "      print(\"Experiment type: \"+experiment_type);\n",
        "      print(\"Model depth: \"+ str(depth));\n",
        "      print(\"Speck reduced to \"+str(num_rounds) +\" R\");\n",
        "\n",
        "      pruning_percentages_one_shot = list(np.linspace(start=0.1,stop=1,num=num_pruning_trials+1))[0:-1];\n",
        "      pruning_percentages_iterative = compute_pruning_percentages_iterative(num_pruning_trials, prune_percentage_for_iterative);\n",
        "\n",
        "      b_accs0 =[];\n",
        "      b_accs0err=[[],[]];\n",
        "      b_tprs0 =[];\n",
        "      b_tprs0err = [[],[]];\n",
        "      b_tnrs0 = [];\n",
        "      b_tnrs0err = [[],[]]; \n",
        "        \n",
        "        \n",
        "      b_accs0 = [np.mean(b_accs)] *  num_pruning_trials;\n",
        "      b_accs0err[0] = [np.mean(b_accs)-np.min(b_accs)] * num_pruning_trials;\n",
        "      b_accs0err[1] = [np.max(b_accs) -np.mean(b_accs)] * num_pruning_trials;\n",
        "\n",
        "      b_tprs0 = [np.mean(b_tprs)] * num_pruning_trials;\n",
        "      b_tprs0err[0] = [np.mean(b_tprs)-np.min(b_tprs)]* num_pruning_trials;\n",
        "      b_tprs0err[1] = [np.max(b_tprs) -np.mean(b_tprs)]* num_pruning_trials;\n",
        "\n",
        "      b_tnrs0 = [np.mean(b_tnrs)] * num_pruning_trials;\n",
        "      b_tnrs0err[0] = [np.mean(b_tnrs)-np.min(b_tnrs)] * num_pruning_trials;\n",
        "      b_tnrs0err[1] = [np.max(b_tnrs)-np.mean(b_tnrs)] * num_pruning_trials;\n",
        "      \n",
        "\n",
        "      os_init_accs1= [];\n",
        "      os_init_accs1err= [[],[]];\n",
        "\n",
        "      os_init_tprs1= [];\n",
        "      os_init_tprs1err= [[],[]];\n",
        "\n",
        "      os_init_tnrs1= [];\n",
        "      os_init_tnrs1err= [[],[]];\n",
        "      \n",
        "\n",
        "      it_init_accs3= [];\n",
        "      it_init_accs3err= [[],[]];\n",
        "\n",
        "      it_init_tprs3= [];\n",
        "      it_init_tprs3err= [[],[]];\n",
        "\n",
        "      it_init_tnrs3= [];\n",
        "      it_init_tnrs3err= [[],[]];\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      for repetition in range(0, num_pruning_trials):\n",
        "        \n",
        "          os_init_accs1.append(np.mean(os_init_accs[repetition]));\n",
        "          os_init_accs1err[0].append(np.mean(os_init_accs[repetition])- np.min(os_init_accs[repetition]));\n",
        "          os_init_accs1err[1].append(np.max(os_init_accs[repetition]) - np.mean(os_init_accs[repetition]));\n",
        "\n",
        "          os_init_tprs1.append(np.mean(os_init_tprs[repetition]));\n",
        "          os_init_tprs1err[0].append(np.mean(os_init_tprs[repetition]) - np.min(os_init_tprs[repetition]));\n",
        "          os_init_tprs1err[1].append(np.max(os_init_tprs[repetition]) - np.mean(os_init_tprs[repetition]));\n",
        "\n",
        "          os_init_tnrs1.append(np.mean(os_init_tnrs[repetition]));\n",
        "          os_init_tnrs1err[0].append(np.mean(os_init_tnrs[repetition])- np.min(os_init_tnrs[repetition]));\n",
        "          os_init_tnrs1err[1].append(np.max(os_init_tnrs[repetition]) -np.mean(os_init_tnrs[repetition]));\n",
        "        \n",
        "\n",
        "          it_init_accs3.append(np.mean(it_init_accs[repetition]));\n",
        "          it_init_accs3err[0].append(np.mean(it_init_accs[repetition]) - np.min(it_init_accs[repetition]));\n",
        "          it_init_accs3err[1].append(np.max(it_init_accs[repetition]) - np.mean(it_init_accs[repetition]));\n",
        "\n",
        "          it_init_tprs3.append(np.mean(it_init_accs[repetition]));\n",
        "          it_init_tprs3err[0].append(np.mean(it_init_accs[repetition])-np.min(it_init_accs[repetition]));\n",
        "          it_init_tprs3err[1].append(np.max(it_init_accs[repetition]) - np.mean(it_init_accs[repetition]));\n",
        "\n",
        "          it_init_tnrs3.append(np.mean(it_init_tnrs[repetition]));\n",
        "          it_init_tnrs3err[0].append(np.mean(it_init_tnrs[repetition])-np.min(it_init_tnrs[repetition]));\n",
        "          it_init_tnrs3err[1].append(np.max(it_init_tnrs[repetition]) - np.mean(it_init_tnrs[repetition]));\n",
        "\n",
        "\n",
        "   \n",
        "      create_figure(\"Accuracies-\"+\"R:\"+str(num_rounds)+\"-\"+experiment_type,\"Pruned ratio\", \"Accuracy\", pruning_percentages_one_shot, b_accs0, b_accs0err, pruning_percentages_one_shot, os_init_accs1, os_init_accs1err, pruning_percentages_iterative, it_init_accs3, it_init_accs3err);\n",
        "      create_figure(\"TPRS-\"+\"R:\"+str(num_rounds)+\"-\"+experiment_type,\"Pruned ratio\", \"TPR\", pruning_percentages_one_shot, b_tprs0, b_tprs0err, pruning_percentages_one_shot, os_init_tprs1, os_init_tprs1err, pruning_percentages_iterative, it_init_tprs3, it_init_tprs3err);\n",
        "      create_figure(\"TNRS-\"+\"R:\"+str(num_rounds) +\"-\"+experiment_type,\"Pruned ratio\", \"TNR\", pruning_percentages_one_shot, b_tnrs0, b_tnrs0err, pruning_percentages_one_shot, os_init_tnrs1, os_init_tnrs1err, pruning_percentages_iterative, it_init_tnrs3, it_init_tnrs3err);\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLUh_AOKni9d"
      },
      "source": [
        "# Running the experiment #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6rq81yHruFy"
      },
      "source": [
        "num_rounds=5; # 6, 7, 8\n",
        "num_epochs=20;\n",
        "num_experiment_repetitions=5;\n",
        "num_pruning_trials= 9;\n",
        "depth=10; # 1\n",
        "data_for_experiment=\"Real_vs_Random\";\n",
        "experiment_type= data_for_experiment;\n",
        "prune_strategy=\"smallest_weights_global\";\n",
        "prune_percentage_for_iterative=0.2;\n",
        "     "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdqzYEwsrkA2"
      },
      "source": [
        "(   b_accs, b_tprs, b_tnrs,\n",
        "    os_init_accs, os_init_tprs, os_init_tnrs,\n",
        "    it_init_accs, it_init_tprs, it_init_tnrs) = execute_experiments(num_rounds, num_epochs, num_experiment_repetitions, num_pruning_trials, depth,data_for_experiment,prune_strategy, prune_percentage_for_iterative);\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVSVmi9yq2qj"
      },
      "source": [
        "create_plots_for_executed_experiments(experiment_type, depth, num_rounds, num_experiment_repetitions, num_pruning_trials, prune_percentage_for_iterative, b_accs, b_tprs, b_tnrs, os_init_accs, os_init_tprs, os_init_tnrs, it_init_accs, it_init_tprs, it_init_tnrs)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}