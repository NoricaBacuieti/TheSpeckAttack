{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DifferentArchitectures_Resnet101V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypPZEY9BuckP"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TfaVe8JWqDc"
      },
      "source": [
        "# The Speck cipher and data generation algorithms #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/speck.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-As2P_AK_gEW"
      },
      "source": [
        "import numpy as np\n",
        "from os import urandom\n",
        "\n",
        "def WORD_SIZE():\n",
        "    return(16);\n",
        "\n",
        "def ALPHA():\n",
        "    return(7);\n",
        "\n",
        "def BETA():\n",
        "    return(2);\n",
        "\n",
        "MASK_VAL = 2 ** WORD_SIZE() - 1;\n",
        "\n",
        "def shuffle_together(l):\n",
        "    state = np.random.get_state();\n",
        "    for x in l:\n",
        "        np.random.set_state(state);\n",
        "        np.random.shuffle(x);\n",
        "\n",
        "def rol(x,k):\n",
        "    return(((x << k) & MASK_VAL) | (x >> (WORD_SIZE() - k)));\n",
        "\n",
        "def ror(x,k):\n",
        "    return((x >> k) | ((x << (WORD_SIZE() - k)) & MASK_VAL));\n",
        "\n",
        "def enc_one_round(p, k):\n",
        "    c0, c1 = p[0], p[1];\n",
        "    c0 = ror(c0, ALPHA());\n",
        "    c0 = (c0 + c1) & MASK_VAL;\n",
        "    c0 = c0 ^ k;\n",
        "    c1 = rol(c1, BETA());\n",
        "    c1 = c1 ^ c0;\n",
        "    return(c0,c1);\n",
        "\n",
        "def dec_one_round(c,k):\n",
        "    c0, c1 = c[0], c[1];\n",
        "    c1 = c1 ^ c0;\n",
        "    c1 = ror(c1, BETA());\n",
        "    c0 = c0 ^ k;\n",
        "    c0 = (c0 - c1) & MASK_VAL;\n",
        "    c0 = rol(c0, ALPHA());\n",
        "    return(c0, c1);\n",
        "\n",
        "def expand_key(k, t):\n",
        "    ks = [0 for i in range(t)];\n",
        "    ks[0] = k[len(k)-1];\n",
        "    l = list(reversed(k[:len(k)-1]));\n",
        "    for i in range(t-1):\n",
        "        l[i%3], ks[i+1] = enc_one_round((l[i%3], ks[i]), i);\n",
        "    return(ks);\n",
        "\n",
        "def encrypt(p, ks):\n",
        "    x, y = p[0], p[1];\n",
        "    for k in ks:\n",
        "        x,y = enc_one_round((x,y), k);\n",
        "    return(x, y);\n",
        "\n",
        "def decrypt(c, ks):\n",
        "    x, y = c[0], c[1];\n",
        "    for k in reversed(ks):\n",
        "        x, y = dec_one_round((x,y), k);\n",
        "    return(x,y);\n",
        "\n",
        "def check_testvector():\n",
        "  key = (0x1918,0x1110,0x0908,0x0100)\n",
        "  pt = (0x6574, 0x694c)\n",
        "  ks = expand_key(key, 22)\n",
        "  ct = encrypt(pt, ks)\n",
        "  if (ct == (0xa868, 0x42f2)):\n",
        "    print(\"Testvector verified.\")\n",
        "    return(True);\n",
        "  else:\n",
        "    print(\"Testvector not verified.\")\n",
        "    return(False);\n",
        "\n",
        "#convert_to_binary takes as input an array of ciphertext pairs\n",
        "#where the first row of the array contains the lefthand side of the ciphertexts,\n",
        "#the second row contains the righthand side of the ciphertexts,\n",
        "#the third row contains the lefthand side of the second ciphertexts,\n",
        "#and so on\n",
        "#it returns an array of bit vectors containing the same data\n",
        "def convert_to_binary(arr):\n",
        "  X = np.zeros((4 * WORD_SIZE(),len(arr[0])),dtype=np.uint8);\n",
        "  for i in range(4 * WORD_SIZE()):\n",
        "    index = i // WORD_SIZE();\n",
        "    offset = WORD_SIZE() - (i % WORD_SIZE()) - 1;\n",
        "    X[i] = (arr[index] >> offset) & 1;\n",
        "  X = X.transpose();\n",
        "  return(X);\n",
        "\n",
        "#takes a text file that contains encrypted block0, block1, true diff prob, real or random\n",
        "#data samples are line separated, the above items whitespace-separated\n",
        "#returns train data, ground truth, optimal ddt prediction\n",
        "def readcsv(datei):\n",
        "    data = np.genfromtxt(datei, delimiter=' ', converters={x: lambda s: int(s,16) for x in range(2)});\n",
        "    X0 = [data[i][0] for i in range(len(data))];\n",
        "    X1 = [data[i][1] for i in range(len(data))];\n",
        "    Y = [data[i][3] for i in range(len(data))];\n",
        "    Z = [data[i][2] for i in range(len(data))];\n",
        "    ct0a = [X0[i] >> 16 for i in range(len(data))];\n",
        "    ct1a = [X0[i] & MASK_VAL for i in range(len(data))];\n",
        "    ct0b = [X1[i] >> 16 for i in range(len(data))];\n",
        "    ct1b = [X1[i] & MASK_VAL for i in range(len(data))];\n",
        "    ct0a = np.array(ct0a, dtype=np.uint16); ct1a = np.array(ct1a,dtype=np.uint16);\n",
        "    ct0b = np.array(ct0b, dtype=np.uint16); ct1b = np.array(ct1b, dtype=np.uint16);\n",
        "    \n",
        "    #X = [[X0[i] >> 16, X0[i] & 0xffff, X1[i] >> 16, X1[i] & 0xffff] for i in range(len(data))];\n",
        "    X = convert_to_binary([ct0a, ct1a, ct0b, ct1b]); \n",
        "    Y = np.array(Y, dtype=np.uint8); Z = np.array(Z);\n",
        "    return(X,Y,Z);\n",
        "\n",
        "#baseline training data generator\n",
        "def make_train_data(n, nr, diff=(0x0040,0)):\n",
        "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
        "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
        "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
        "  num_rand_samples = np.sum(Y==0);\n",
        "  plain1l[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  plain1r[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  ks = expand_key(keys, nr);\n",
        "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
        "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
        "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
        "  return(X,Y);\n",
        "\n",
        "#real differences data generator\n",
        "def real_differences_data(n, nr, diff=(0x0040,0)):\n",
        "  #generate labels\n",
        "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
        "  #generate keys\n",
        "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
        "  #generate plaintexts\n",
        "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  #apply input difference\n",
        "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
        "  num_rand_samples = np.sum(Y==0);\n",
        "  #expand keys and encrypt\n",
        "  ks = expand_key(keys, nr);\n",
        "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
        "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
        "  #generate blinding values\n",
        "  k0 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  k1 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  #apply blinding to the samples labelled as random\n",
        "  ctdata0l[Y==0] = ctdata0l[Y==0] ^ k0; ctdata0r[Y==0] = ctdata0r[Y==0] ^ k1;\n",
        "  ctdata1l[Y==0] = ctdata1l[Y==0] ^ k0; ctdata1r[Y==0] = ctdata1r[Y==0] ^ k1;\n",
        "  #convert to input data for neural networks\n",
        "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
        "  return(X,Y);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwEv06GbWrm0"
      },
      "source": [
        "# Evaluate the results #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/eval.py and slightly adapted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JUEUGNtKmM_"
      },
      "source": [
        "def evaluate(net,X,Y):\n",
        "    Z = net.predict(X,batch_size=10000).flatten();\n",
        "    Zbin = (Z > 0.5);\n",
        "    diff = Y - Z; \n",
        "    mse = np.mean(diff*diff);\n",
        "    n = len(Z); n0 = np.sum(Y==0); n1 = np.sum(Y==1);\n",
        "    acc = np.sum(Zbin == Y) / n;\n",
        "    tpr = np.sum(Zbin[Y==1]) / n1;\n",
        "    tnr = np.sum(Zbin[Y==0] == 0) / n0;\n",
        "    mreal = np.median(Z[Y==1]);\n",
        "    high_random = np.sum(Z[Y==0] > mreal) / n0;\n",
        "\n",
        "    return(acc, tpr, tnr); # added\n",
        "    #print(\"Accuracy: \", acc, \"TPR: \", tpr, \"TNR: \", tnr, \"MSE:\", mse);\n",
        "    #print(\"Percentage of random pairs with score higher than median of real pairs:\", 100*high_random);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e11zbKiWuOL"
      },
      "source": [
        "# Conducting multiple evaluations #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOxHHI6UKnpf"
      },
      "source": [
        "def multiple_evaluations(model, repetitions, num_rounds):\n",
        " \n",
        "  accs = [];\n",
        "  tprs = [];\n",
        "  tnrs = [];\n",
        "  for i in range(0, repetitions):\n",
        "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "\n",
        "    (acc, tpr, tnr) = evaluate(model, X_eval, Y_eval);\n",
        "    accs.append(acc);\n",
        "    tprs.append(tpr);\n",
        "    tnrs.append(tnr);\n",
        "\n",
        "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
        "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
        "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
        "        \n",
        "  return(np.mean(accs), np.mean(tprs), np.mean(tnrs));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDMSF5HwWxbG"
      },
      "source": [
        "# ResNet101V2 network#\n",
        "Taken from https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/applications/resnet_v2.py#L59-L87 and adapted to this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUytQ7hAvt-H"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Conv1D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, Concatenate,Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.regularizers import l2, l1, l1_l2\n",
        "from keras import layers;\n",
        "\n",
        "bs = 5000;\n",
        "\n",
        "def cyclic_lr(num_epochs, high_lr, low_lr):\n",
        "  res = lambda i: low_lr + ((num_epochs-1) - i % num_epochs)/(num_epochs-1) * (high_lr - low_lr);\n",
        "  return(res);\n",
        "\n",
        "def make_checkpoint_best_worst(datei):\n",
        "  res = ModelCheckpoint(datei, monitor='val_acc', save_best_only = True, save_weights_only=True );\n",
        "  return(res);\n",
        "\n",
        "def make_checkpoint_all(datei):\n",
        "  res = ModelCheckpoint(datei, monitor='val_acc', save_best_only = False, save_weights_only=True);\n",
        "  return(res);\n",
        "\n",
        "def block(x, filters, kernel_size=3, stride=1, conv_shortcut=False):\n",
        "\n",
        "\n",
        "  preact = layers.BatchNormalization()(x)\n",
        "  preact = layers.Activation('relu')(preact)\n",
        "\n",
        "  if conv_shortcut:\n",
        "    shortcut = layers.Conv1D(4 * filters, 1, strides=stride)(preact)\n",
        "  else:\n",
        "    shortcut = layers.MaxPooling1D(1, strides=stride)(x) if stride > 1 else x\n",
        "\n",
        "  x = layers.Conv1D(filters, 1, strides=1, use_bias=False)(preact)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation('relu')(x)\n",
        "\n",
        "\n",
        "  x = layers.Conv1D(filters,kernel_size,strides=stride,use_bias=False, padding ='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation('relu')(x)\n",
        "\n",
        "  x = layers.Conv1D(4 * filters, 1)(x)\n",
        "  x = layers.Add()([shortcut, x])\n",
        "  return x\n",
        "\n",
        "\n",
        "\n",
        "def stack(x, filters, blocks, stride1=2):\n",
        "\n",
        "  x = block(x, filters, conv_shortcut=True)\n",
        "\n",
        "  for i in range(2, blocks):\n",
        "    x = block(x, filters)\n",
        "  x = block(x, filters, stride=stride1)\n",
        "  return x\n",
        "\n",
        "def stack_fn(x):\n",
        "    x = stack(x, 64, 3)\n",
        "    x = stack(x, 128, 4)\n",
        "    x = stack(x, 256, 23)\n",
        "    return stack(x, 512, 3, stride1=1)\n",
        "\n",
        "\n",
        "def ResNet(do_pooling = True):\n",
        "\n",
        "  \n",
        "\n",
        "  input = Input(shape=(64,));\n",
        "  x = Reshape((4, 16))(input);\n",
        "  x = Permute((2,1))(x);\n",
        "\n",
        "\n",
        "  x = layers.Conv1D(64, 1, strides=1)(x)\n",
        "  x = layers.BatchNormalization( )(x)\n",
        "  x = layers.Activation('relu')(x)\n",
        "\n",
        "\n",
        "  if do_pooling:\n",
        "    x = layers.MaxPooling1D(3, strides=2)(x)  \n",
        "\n",
        "\n",
        "  x = stack_fn(x)\n",
        "\n",
        "\n",
        "  if do_pooling:\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        \n",
        "        \n",
        "  x = Flatten()(x);\n",
        "  x = layers.Dense(1000)(x)\n",
        "  x = layers.BatchNormalization()(x);\n",
        "  x = layers.Activation('relu')(x);\n",
        "    \n",
        "  output = layers.Dense(1, activation='sigmoid')(x)\n",
        "  model = Model(input, output)\n",
        "\n",
        "\n",
        "  return model\n",
        "\n",
        "def model_builder(do_pooling):\n",
        "\n",
        "\n",
        "\n",
        "  model = ResNet(do_pooling = do_pooling);\n",
        "  model.compile(\n",
        "          optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc']);\n",
        "  return model;\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def train_speck_distinguisher(model, num_epochs, num_rounds, X_train, Y_train, X_eval, Y_eval):\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights= True);\n",
        "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
        "    history = model.fit(X_train, Y_train, batch_size= bs, epochs=num_epochs,validation_data=(X_eval, Y_eval), callbacks=[lr, stop_early])\n",
        "    \n",
        "    return(model);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScRo3krPW5yi"
      },
      "source": [
        "# Repeat the experiment #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37RWop41vt-J"
      },
      "source": [
        "def repeat_experiment(do_pooling, num_reps, num_rounds):\n",
        "  accs= [];\n",
        "  tprs =[];\n",
        "  tnrs=[];\n",
        "\n",
        "  for i in range(0, num_reps):\n",
        "    model = model_builder(do_pooling);\n",
        "    X_train, Y_train = make_train_data(10**7,num_rounds);\n",
        "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "    trained_model = train_speck_distinguisher(model, 30, num_rounds, X_train, Y_train, X_eval, Y_eval);\n",
        "    (acc, tpr, tnr) = multiple_evaluations(trained_model, 5, num_rounds);\n",
        "    accs.append(acc);\n",
        "    tprs.append(tpr);\n",
        "    tnrs.append(tnr);\n",
        "\n",
        "  print(\"Round: \"+str(num_rounds));\n",
        "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
        "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
        "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
        "  return (accs, tprs, tnrs);\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD5lolTevt-K",
        "scrolled": true
      },
      "source": [
        "repeat_experiment(do_pooling = True, num_reps = 1, num_rounds= 5)\n",
        "# do_pooling = True,False\n",
        "# num_reps = of your choosing\n",
        "# num_rounds = 5, 6, 7, 8"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}