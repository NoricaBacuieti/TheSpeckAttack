{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TheSpeckAttack_FeatureVisualization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_NClEn2y-EJ"
      },
      "source": [
        "!pip install lime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypPZEY9BuckP"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.ensemble\n",
        "import numpy as np\n",
        "import lime\n",
        "import lime.lime_tabular"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYrc0_7kKfpr"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Conv1D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, Concatenate,Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, UpSampling1D\n",
        "from keras.regularizers import l2, l1, l1_l2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_DwsHjRy0x7"
      },
      "source": [
        "# The Speck cipher and data generation algorithms #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/speck.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-As2P_AK_gEW"
      },
      "source": [
        "import numpy as np\n",
        "from os import urandom\n",
        "\n",
        "def WORD_SIZE():\n",
        "    return(16);\n",
        "\n",
        "def ALPHA():\n",
        "    return(7);\n",
        "\n",
        "def BETA():\n",
        "    return(2);\n",
        "\n",
        "MASK_VAL = 2 ** WORD_SIZE() - 1;\n",
        "\n",
        "def shuffle_together(l):\n",
        "    state = np.random.get_state();\n",
        "    for x in l:\n",
        "        np.random.set_state(state);\n",
        "        np.random.shuffle(x);\n",
        "\n",
        "def rol(x,k):\n",
        "    return(((x << k) & MASK_VAL) | (x >> (WORD_SIZE() - k)));\n",
        "\n",
        "def ror(x,k):\n",
        "    return((x >> k) | ((x << (WORD_SIZE() - k)) & MASK_VAL));\n",
        "\n",
        "def enc_one_round(p, k):\n",
        "    c0, c1 = p[0], p[1];\n",
        "    c0 = ror(c0, ALPHA());\n",
        "    c0 = (c0 + c1) & MASK_VAL;\n",
        "    c0 = c0 ^ k;\n",
        "    c1 = rol(c1, BETA());\n",
        "    c1 = c1 ^ c0;\n",
        "    return(c0,c1);\n",
        "\n",
        "def dec_one_round(c,k):\n",
        "    c0, c1 = c[0], c[1];\n",
        "    c1 = c1 ^ c0;\n",
        "    c1 = ror(c1, BETA());\n",
        "    c0 = c0 ^ k;\n",
        "    c0 = (c0 - c1) & MASK_VAL;\n",
        "    c0 = rol(c0, ALPHA());\n",
        "    return(c0, c1);\n",
        "\n",
        "def expand_key(k, t):\n",
        "    ks = [0 for i in range(t)];\n",
        "    ks[0] = k[len(k)-1];\n",
        "    l = list(reversed(k[:len(k)-1]));\n",
        "    for i in range(t-1):\n",
        "        l[i%3], ks[i+1] = enc_one_round((l[i%3], ks[i]), i);\n",
        "    return(ks);\n",
        "\n",
        "def encrypt(p, ks):\n",
        "    x, y = p[0], p[1];\n",
        "    for k in ks:\n",
        "        x,y = enc_one_round((x,y), k);\n",
        "    return(x, y);\n",
        "\n",
        "def decrypt(c, ks):\n",
        "    x, y = c[0], c[1];\n",
        "    for k in reversed(ks):\n",
        "        x, y = dec_one_round((x,y), k);\n",
        "    return(x,y);\n",
        "\n",
        "def check_testvector():\n",
        "  key = (0x1918,0x1110,0x0908,0x0100)\n",
        "  pt = (0x6574, 0x694c)\n",
        "  ks = expand_key(key, 22)\n",
        "  ct = encrypt(pt, ks)\n",
        "  if (ct == (0xa868, 0x42f2)):\n",
        "    print(\"Testvector verified.\")\n",
        "    return(True);\n",
        "  else:\n",
        "    print(\"Testvector not verified.\")\n",
        "    return(False);\n",
        "\n",
        "#convert_to_binary takes as input an array of ciphertext pairs\n",
        "#where the first row of the array contains the lefthand side of the ciphertexts,\n",
        "#the second row contains the righthand side of the ciphertexts,\n",
        "#the third row contains the lefthand side of the second ciphertexts,\n",
        "#and so on\n",
        "#it returns an array of bit vectors containing the same data\n",
        "def convert_to_binary(arr):\n",
        "  X = np.zeros((4 * WORD_SIZE(),len(arr[0])),dtype=np.uint8);\n",
        "  for i in range(4 * WORD_SIZE()):\n",
        "    index = i // WORD_SIZE();\n",
        "    offset = WORD_SIZE() - (i % WORD_SIZE()) - 1;\n",
        "    X[i] = (arr[index] >> offset) & 1;\n",
        "  X = X.transpose();\n",
        "  return(X);\n",
        "\n",
        "#takes a text file that contains encrypted block0, block1, true diff prob, real or random\n",
        "#data samples are line separated, the above items whitespace-separated\n",
        "#returns train data, ground truth, optimal ddt prediction\n",
        "def readcsv(datei):\n",
        "    data = np.genfromtxt(datei, delimiter=' ', converters={x: lambda s: int(s,16) for x in range(2)});\n",
        "    X0 = [data[i][0] for i in range(len(data))];\n",
        "    X1 = [data[i][1] for i in range(len(data))];\n",
        "    Y = [data[i][3] for i in range(len(data))];\n",
        "    Z = [data[i][2] for i in range(len(data))];\n",
        "    ct0a = [X0[i] >> 16 for i in range(len(data))];\n",
        "    ct1a = [X0[i] & MASK_VAL for i in range(len(data))];\n",
        "    ct0b = [X1[i] >> 16 for i in range(len(data))];\n",
        "    ct1b = [X1[i] & MASK_VAL for i in range(len(data))];\n",
        "    ct0a = np.array(ct0a, dtype=np.uint16); ct1a = np.array(ct1a,dtype=np.uint16);\n",
        "    ct0b = np.array(ct0b, dtype=np.uint16); ct1b = np.array(ct1b, dtype=np.uint16);\n",
        "    \n",
        "    #X = [[X0[i] >> 16, X0[i] & 0xffff, X1[i] >> 16, X1[i] & 0xffff] for i in range(len(data))];\n",
        "    X = convert_to_binary([ct0a, ct1a, ct0b, ct1b]); \n",
        "    Y = np.array(Y, dtype=np.uint8); Z = np.array(Z);\n",
        "    return(X,Y,Z);\n",
        "\n",
        "#baseline training data generator\n",
        "def make_train_data(n, nr, diff=(0x0040,0)):\n",
        "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
        "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
        "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
        "  num_rand_samples = np.sum(Y==0);\n",
        "  plain1l[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  plain1r[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  ks = expand_key(keys, nr);\n",
        "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
        "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
        "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
        "  return(X,Y);\n",
        "\n",
        "#real differences data generator\n",
        "def real_differences_data(n, nr, diff=(0x0040,0)):\n",
        "  #generate labels\n",
        "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
        "  #generate keys\n",
        "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
        "  #generate plaintexts\n",
        "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  #apply input difference\n",
        "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
        "  num_rand_samples = np.sum(Y==0);\n",
        "  #expand keys and encrypt\n",
        "  ks = expand_key(keys, nr);\n",
        "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
        "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
        "  #generate blinding values\n",
        "  k0 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  k1 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  #apply blinding to the samples labelled as random\n",
        "  ctdata0l[Y==0] = ctdata0l[Y==0] ^ k0; ctdata0r[Y==0] = ctdata0r[Y==0] ^ k1;\n",
        "  ctdata1l[Y==0] = ctdata1l[Y==0] ^ k0; ctdata1r[Y==0] = ctdata1r[Y==0] ^ k1;\n",
        "  #convert to input data for neural networks\n",
        "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
        "  return(X,Y);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKhJndYPzDtf"
      },
      "source": [
        "# The depth-1/10 distinguisher implementation - Pruned version#\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/train_nets.py and slightly adapted for running multiple trials. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STtCeSLx8iCt"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Conv1D,Conv2D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, Concatenate,Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, UpSampling1D\n",
        "from keras.regularizers import l2, l1, l1_l2\n",
        "\n",
        "\n",
        "def cyclic_lr(num_epochs, high_lr, low_lr):\n",
        "  res = lambda i: low_lr + ((num_epochs-1) - i % num_epochs)/(num_epochs-1) * (high_lr - low_lr);\n",
        "  return(res);\n",
        "\n",
        "def make_checkpoint_best_worst(datei):\n",
        "  res = ModelCheckpoint(datei, monitor='val_acc', save_best_only = True, save_weights_only=True );\n",
        "  return(res);\n",
        "\n",
        "def make_checkpoint_all(datei):\n",
        "  res = ModelCheckpoint(datei, monitor='val_acc', save_best_only = False, save_weights_only=True);\n",
        "  return(res);\n",
        "\n",
        "\n",
        "wdir_curent_bw = \"./current_bw/\";\n",
        "wdir_curent_bw_all= \"./current_bw_all/\";\n",
        "\n",
        "bs = 5000;\n",
        "\n",
        "def make_resnet( num_blocks=2, num_filters=32, num_outputs=1, d1=64, d2=64, word_size=16, ks=3,depth=5, reg_param=0.0001, final_activation='sigmoid'):\n",
        "  #Input and preprocessing layers\n",
        "  #Input and preprocessing layers\n",
        "\n",
        "  #input is 10^7 samples of 64 bits each\n",
        "  inp = Input(shape=(64,));\n",
        "  x = Reshape((4, 16))(inp);\n",
        "  x = Permute((2,1))(x);\n",
        "  \n",
        "  conv0 = Conv1D(25, kernel_size=1, padding='same')(x);\n",
        "  conv0 = BatchNormalization()(conv0);\n",
        "  conv0 = Activation('relu')(conv0);\n",
        "\n",
        "  conv1 = Conv1D(11, kernel_size=ks, padding='same')(conv0);\n",
        "  conv1 = BatchNormalization()(conv1);\n",
        "  conv1 = Activation('relu')(conv1);\n",
        "\n",
        "  conv2 = Conv1D(8, kernel_size=ks, padding='same')(conv1);\n",
        "  conv2 = BatchNormalization()(conv2);\n",
        "  conv2 = Activation('relu')(conv2);\n",
        "\n",
        "  flat = Flatten()(conv2);\n",
        "\n",
        "  dense1 = Dense(18)(flat);\n",
        "  dense1 = BatchNormalization()(dense1);\n",
        "  dense1 = Activation('relu')(dense1);\n",
        "  dense2 = Dense(28)(dense1);\n",
        "  dense2 = BatchNormalization()(dense2);\n",
        "  dense2 = Activation('relu')(dense2);\n",
        "  out = Dense(1, activation='sigmoid')(dense2);\n",
        "  model = Model(inputs=inp, outputs=out);\n",
        "\n",
        "  return model\n",
        "\n",
        "def model_builder(depth):\n",
        "\n",
        "  model = make_resnet(depth=depth);\n",
        "  model.compile(\n",
        "          optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc']);\n",
        "  return model;\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def train_speck_distinguisher(model, num_epochs, num_rounds, X_train, Y_train, X_eval, Y_eval):\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights= True);\n",
        "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
        "    history = model.fit(X_train, Y_train, batch_size= bs, epochs=num_epochs,validation_data=(X_eval, Y_eval), callbacks=[lr, stop_early])\n",
        "    \n",
        "    return(model);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts3UqJ9qzllx"
      },
      "source": [
        "Names for the 64 bits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6hRwg4VyXuC"
      },
      "source": [
        "def generate_names():\n",
        "  names =[];\n",
        "  for i in range(64):\n",
        "    names.append(\"B\"+str(i));\n",
        "\n",
        "  return names;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9ZXAll8yXuC"
      },
      "source": [
        "feature_names = generate_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt3hr_YbyXuD"
      },
      "source": [
        "# Train distinguisher #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aZHcTur789i0"
      },
      "source": [
        "num_rounds = 5; # 6, 7, 8\n",
        "\n",
        "X_train, Y_train = make_train_data(10**7, num_rounds);\n",
        "X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "\n",
        "initial_model = model_builder(1);\n",
        "trained_model= train_speck_distinguisher(initial_model, 30, num_rounds, X_train, Y_train, X_eval, Y_eval);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqqvc4Ikz2ko"
      },
      "source": [
        "# Find samples with best/worst prediction #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyPAIY31yXuE"
      },
      "source": [
        "def find_best_preds(trained_model, num_preds, num_rounds):\n",
        "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "    predictions= trained_model.predict(X_eval, batch_size=5000)\n",
        "    stored_0 =[];\n",
        "    stored_1 =[];\n",
        "    \n",
        "    for i in range(10**6):\n",
        "        diff = abs(Y_eval[i] - predictions[i]);\n",
        "        if Y_eval[i] ==1 and (len(stored_1)==0 or diff <= stored_1[len(stored_1)-1][1]):\n",
        "            stored_1.append((i, diff, Y_eval[i], predictions[i], X_eval[i]));\n",
        "        if Y_eval[i] ==0 and (len(stored_0)==0 or diff <= stored_0[len(stored_0)-1][1]):\n",
        "            stored_0.append((i, diff, Y_eval[i], predictions[i], X_eval[i]));\n",
        "    return stored_1[-num_preds:], stored_0[-num_preds:];\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDumbBAByXuE"
      },
      "source": [
        "def find_worst_preds(trained_model, num_preds, num_rounds):\n",
        "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "    predictions= trained_model.predict(X_eval, batch_size=5000)\n",
        "    stored_0 =[];\n",
        "    stored_1 =[];\n",
        "    \n",
        "    for i in range(10**6):\n",
        "        diff = abs(Y_eval[i] - predictions[i]);\n",
        "        if Y_eval[i] ==1 and (len(stored_1)==0 or diff >= stored_1[len(stored_1)-1][1]):\n",
        "            stored_1.append((i, diff, Y_eval[i], predictions[i], X_eval[i]));\n",
        "        if Y_eval[i] ==0 and (len(stored_0)==0 or diff >= stored_0[len(stored_0)-1][1]):\n",
        "            stored_0.append((i, diff, Y_eval[i], predictions[i], X_eval[i]));\n",
        "    return stored_1[-num_preds:], stored_0[-num_preds:];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvzRGnffyXuE"
      },
      "source": [
        "s1, s2 = find_best_preds(trained_model, 5, 5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK4mM1U1z-dR"
      },
      "source": [
        "# Instantiate the explainer #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jZ523ViyXuF"
      },
      "source": [
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, class_names= ['Fixed difference'], feature_names=feature_names,discretize_continuous=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykJy_Gp8yXuF"
      },
      "source": [
        "Explanations for class 1 - fixed difference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yGB40XJyXuF"
      },
      "source": [
        "exp = explainer.explain_instance(s1[0][4], trained_model.predict, num_features=64, num_samples=10**7, labels=[0])\n",
        "exp.show_in_notebook(show_table=True, show_all=True)\n",
        "fig = exp.as_pyplot_figure(label=0);\n",
        "fig.set_size_inches(10, 15)\n",
        "#fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPWJmJLCyXuj"
      },
      "source": [
        "Explanations for class 0 - random difference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzCbZ2lyyXuj"
      },
      "source": [
        "exp = explainer.explain_instance(s2[0][4], trained_model.predict, num_features=64, num_samples=10**7, labels=[0])\n",
        "exp.show_in_notebook(show_table=True, show_all=True)\n",
        "fig = exp.as_pyplot_figure(label=0);\n",
        "fig.set_size_inches(10, 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEts9lZ_yXyg"
      },
      "source": [
        "# Submodular Explainer #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTiRBFBUyXyh"
      },
      "source": [
        "num_rounds = 5; # 6, 7, 8\n",
        "\n",
        "X_train, Y_train = make_train_data(10**7, num_rounds);\n",
        "X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "\n",
        "initial_model = model_builder(1);\n",
        "trained_model= train_speck_distinguisher(initial_model, 30, num_rounds, X_train, Y_train, X_eval, Y_eval);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPkpz3W4yXyh"
      },
      "source": [
        "import warnings\n",
        "from lime import submodular_pick\n",
        "sp_obj = submodular_pick.SubmodularPick(explainer, X_train, trained_model.predict, method='sample', num_features=64, num_exps_desired=5,  sample_size =5000) \n",
        "[exp.as_pyplot_figure(label=exp.available_labels()[0]).set_size_inches(10, 15) for exp in sp_obj.sp_explanations];"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahFpjvRVyXyj"
      },
      "source": [
        "[exp.show_in_notebook(show_table=True, show_all=True) for exp in sp_obj.sp_explanations];"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}