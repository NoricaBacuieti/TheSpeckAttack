{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypPZEY9BuckP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezbWCxTP1W0R"
   },
   "source": [
    "# The Speck cipher and data generation algorithms #\n",
    "Taken from https://github.com/agohr/deep_speck/blob/master/speck.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-As2P_AK_gEW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import urandom\n",
    "\n",
    "def WORD_SIZE():\n",
    "    return(16);\n",
    "\n",
    "def ALPHA():\n",
    "    return(7);\n",
    "\n",
    "def BETA():\n",
    "    return(2);\n",
    "\n",
    "MASK_VAL = 2 ** WORD_SIZE() - 1;\n",
    "\n",
    "def shuffle_together(l):\n",
    "    state = np.random.get_state();\n",
    "    for x in l:\n",
    "        np.random.set_state(state);\n",
    "        np.random.shuffle(x);\n",
    "\n",
    "def rol(x,k):\n",
    "    return(((x << k) & MASK_VAL) | (x >> (WORD_SIZE() - k)));\n",
    "\n",
    "def ror(x,k):\n",
    "    return((x >> k) | ((x << (WORD_SIZE() - k)) & MASK_VAL));\n",
    "\n",
    "def enc_one_round(p, k):\n",
    "    c0, c1 = p[0], p[1];\n",
    "    c0 = ror(c0, ALPHA());\n",
    "    c0 = (c0 + c1) & MASK_VAL;\n",
    "    c0 = c0 ^ k;\n",
    "    c1 = rol(c1, BETA());\n",
    "    c1 = c1 ^ c0;\n",
    "    return(c0,c1);\n",
    "\n",
    "def dec_one_round(c,k):\n",
    "    c0, c1 = c[0], c[1];\n",
    "    c1 = c1 ^ c0;\n",
    "    c1 = ror(c1, BETA());\n",
    "    c0 = c0 ^ k;\n",
    "    c0 = (c0 - c1) & MASK_VAL;\n",
    "    c0 = rol(c0, ALPHA());\n",
    "    return(c0, c1);\n",
    "\n",
    "def expand_key(k, t):\n",
    "    ks = [0 for i in range(t)];\n",
    "    ks[0] = k[len(k)-1];\n",
    "    l = list(reversed(k[:len(k)-1]));\n",
    "    for i in range(t-1):\n",
    "        l[i%3], ks[i+1] = enc_one_round((l[i%3], ks[i]), i);\n",
    "    return(ks);\n",
    "\n",
    "def encrypt(p, ks):\n",
    "    x, y = p[0], p[1];\n",
    "    for k in ks:\n",
    "        x,y = enc_one_round((x,y), k);\n",
    "    return(x, y);\n",
    "\n",
    "def decrypt(c, ks):\n",
    "    x, y = c[0], c[1];\n",
    "    for k in reversed(ks):\n",
    "        x, y = dec_one_round((x,y), k);\n",
    "    return(x,y);\n",
    "\n",
    "def check_testvector():\n",
    "  key = (0x1918,0x1110,0x0908,0x0100)\n",
    "  pt = (0x6574, 0x694c)\n",
    "  ks = expand_key(key, 22)\n",
    "  ct = encrypt(pt, ks)\n",
    "  if (ct == (0xa868, 0x42f2)):\n",
    "    print(\"Testvector verified.\")\n",
    "    return(True);\n",
    "  else:\n",
    "    print(\"Testvector not verified.\")\n",
    "    return(False);\n",
    "\n",
    "#convert_to_binary takes as input an array of ciphertext pairs\n",
    "#where the first row of the array contains the lefthand side of the ciphertexts,\n",
    "#the second row contains the righthand side of the ciphertexts,\n",
    "#the third row contains the lefthand side of the second ciphertexts,\n",
    "#and so on\n",
    "#it returns an array of bit vectors containing the same data\n",
    "def convert_to_binary(arr):\n",
    "  X = np.zeros((4 * WORD_SIZE(),len(arr[0])),dtype=np.uint8);\n",
    "  for i in range(4 * WORD_SIZE()):\n",
    "    index = i // WORD_SIZE();\n",
    "    offset = WORD_SIZE() - (i % WORD_SIZE()) - 1;\n",
    "    X[i] = (arr[index] >> offset) & 1;\n",
    "  X = X.transpose();\n",
    "  return(X);\n",
    "\n",
    "#takes a text file that contains encrypted block0, block1, true diff prob, real or random\n",
    "#data samples are line separated, the above items whitespace-separated\n",
    "#returns train data, ground truth, optimal ddt prediction\n",
    "def readcsv(datei):\n",
    "    data = np.genfromtxt(datei, delimiter=' ', converters={x: lambda s: int(s,16) for x in range(2)});\n",
    "    X0 = [data[i][0] for i in range(len(data))];\n",
    "    X1 = [data[i][1] for i in range(len(data))];\n",
    "    Y = [data[i][3] for i in range(len(data))];\n",
    "    Z = [data[i][2] for i in range(len(data))];\n",
    "    ct0a = [X0[i] >> 16 for i in range(len(data))];\n",
    "    ct1a = [X0[i] & MASK_VAL for i in range(len(data))];\n",
    "    ct0b = [X1[i] >> 16 for i in range(len(data))];\n",
    "    ct1b = [X1[i] & MASK_VAL for i in range(len(data))];\n",
    "    ct0a = np.array(ct0a, dtype=np.uint16); ct1a = np.array(ct1a,dtype=np.uint16);\n",
    "    ct0b = np.array(ct0b, dtype=np.uint16); ct1b = np.array(ct1b, dtype=np.uint16);\n",
    "    \n",
    "    #X = [[X0[i] >> 16, X0[i] & 0xffff, X1[i] >> 16, X1[i] & 0xffff] for i in range(len(data))];\n",
    "    X = convert_to_binary([ct0a, ct1a, ct0b, ct1b]); \n",
    "    Y = np.array(Y, dtype=np.uint8); Z = np.array(Z);\n",
    "    return(X,Y,Z);\n",
    "\n",
    "#baseline training data generator\n",
    "def make_train_data(n, nr, diff=(0x0040,0)):\n",
    "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
    "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
    "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
    "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
    "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
    "  num_rand_samples = np.sum(Y==0);\n",
    "  plain1l[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
    "  plain1r[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
    "  ks = expand_key(keys, nr);\n",
    "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
    "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
    "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
    "  return(X,Y);\n",
    "\n",
    "#real differences data generator\n",
    "def real_differences_data(n, nr, diff=(0x0040,0)):\n",
    "  #generate labels\n",
    "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
    "  #generate keys\n",
    "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
    "  #generate plaintexts\n",
    "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
    "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
    "  #apply input difference\n",
    "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
    "  num_rand_samples = np.sum(Y==0);\n",
    "  #expand keys and encrypt\n",
    "  ks = expand_key(keys, nr);\n",
    "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
    "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
    "  #generate blinding values\n",
    "  k0 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
    "  k1 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
    "  #apply blinding to the samples labelled as random\n",
    "  ctdata0l[Y==0] = ctdata0l[Y==0] ^ k0; ctdata0r[Y==0] = ctdata0r[Y==0] ^ k1;\n",
    "  ctdata1l[Y==0] = ctdata1l[Y==0] ^ k0; ctdata1r[Y==0] = ctdata1r[Y==0] ^ k1;\n",
    "  #convert to input data for neural networks\n",
    "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
    "  return(X,Y);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_RmB6ZT1aDn"
   },
   "source": [
    "# Evaluate the results #\n",
    "Taken from https://github.com/agohr/deep_speck/blob/master/eval.py and slightly adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JUEUGNtKmM_"
   },
   "outputs": [],
   "source": [
    "def evaluate(net,X,Y):\n",
    "    \n",
    "    Z = net.predict(X,batch_size=10000).flatten();\n",
    "    Zbin = (Z > 0.5);\n",
    "    \n",
    "    #Compute the acc, tpr, tnr\n",
    "    n = len(Z); \n",
    "    n0 = np.sum(Y==0); \n",
    "    n1 = np.sum(Y==1);\n",
    "    \n",
    "    acc = np.sum(Zbin == Y) / n;\n",
    "    tpr = np.sum(Zbin[Y==1]) / n1;\n",
    "    tnr = np.sum(Zbin[Y==0] == 0) / n0;\n",
    "    \n",
    "    return(acc, tpr, tnr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnfGSrXE1fGa"
   },
   "source": [
    "# Conduct multiple evaluations #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOxHHI6UKnpf"
   },
   "outputs": [],
   "source": [
    "def multiple_evaluations(model, repetitions, num_rounds):\n",
    " \n",
    "  #The accs, tprs, tnrs for all evaluation repetition\n",
    "  accs = [];\n",
    "  tprs = [];\n",
    "  tnrs = [];\n",
    "    \n",
    "  #Evaluate multiple times and average the results \n",
    "  for i in range(0, repetitions):\n",
    "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
    "\n",
    "    (acc, tpr, tnr) = evaluate(model, X_eval, Y_eval);\n",
    "    accs.append(acc);\n",
    "    tprs.append(tpr);\n",
    "    tnrs.append(tnr);\n",
    "\n",
    "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
    "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
    "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
    "        \n",
    "  return(np.mean(accs), np.mean(tprs), np.mean(tnrs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osZhsMLo1i_9"
   },
   "source": [
    "# The reduced network #\n",
    "Obtained from the depth-1 distinguisher implemented in https://github.com/agohr/deep_speck/blob/master/train_nets.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15v7LVaP1RHX"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv1D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, Concatenate,Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import l2, l1, l1_l2\n",
    "\n",
    "\n",
    "def cyclic_lr(num_epochs, high_lr, low_lr):\n",
    "  res = lambda i: low_lr + ((num_epochs-1) - i % num_epochs)/(num_epochs-1) * (high_lr - low_lr);\n",
    "  return(res);\n",
    "\n",
    "#Batch size\n",
    "bs = 5000;\n",
    "\n",
    "def build_reduced_network():\n",
    "\n",
    "  #Input and preprocessing layers\n",
    "  inp = Input(shape=(64,));\n",
    "  x = Reshape((4, 16))(inp);\n",
    "  x = Permute((2,1))(x);\n",
    "  \n",
    "  #Block 1: 7 filters were pruned \n",
    "  conv1 = Conv1D(filters=25, kernel_size=1, padding='same')(x); #Also refered to as C1\n",
    "  conv1 = BatchNormalization()(conv1);\n",
    "  conv1 = Activation('relu')(conv1);\n",
    "\n",
    "  #Block 2-i where:\n",
    "  #From its first convolutional layer: 21 filters were pruned\n",
    "  conv2 = Conv1D(filters=11, kernel_size=3, padding='same')(conv1); #Also refered to as C2\n",
    "  conv2 = BatchNormalization()(conv2);\n",
    "  conv2 = Activation('relu')(conv2);\n",
    "\n",
    "  #From its second convolutional layer: 25 filters were pruned\n",
    "  conv3 = Conv1D(filters=7, kernel_size=3, padding='same')(conv2); #Also refered to as C3\n",
    "  conv3 = BatchNormalization()(conv3);\n",
    "  conv3 = Activation('relu')(conv3);\n",
    "\n",
    "  #The residual connection was removed \n",
    "    \n",
    "  flat = Flatten()(conv3);\n",
    "\n",
    "  #Block 3 where:\n",
    "  #From its first dense layer: 46 neurons were pruned   \n",
    "  dense1 = Dense(18)(flat); #Also refered to as D1\n",
    "  dense1 = BatchNormalization()(dense1);\n",
    "  dense1 = Activation('relu')(dense1);\n",
    "    \n",
    "  #From its second dense layer: 36 neurons were pruned  \n",
    "  dense2 = Dense(28)(dense1); #Also refered to as D2\n",
    "  dense2 = BatchNormalization()(dense2);\n",
    "  dense2 = Activation('relu')(dense2);\n",
    "\n",
    "  out = Dense(1, activation='sigmoid')(dense2);\n",
    "    \n",
    "  model = Model(inputs=inp, outputs=out);\n",
    "\n",
    "  return model;\n",
    "\n",
    "\n",
    "\n",
    "def model_builder():\n",
    "\n",
    "  model = build_reduced_network();\n",
    "\n",
    "  model.compile(\n",
    "          optimizer='adam',\n",
    "          loss='binary_crossentropy',\n",
    "          metrics=['acc']);\n",
    "    \n",
    "  return model;\n",
    "  \n",
    "\n",
    "\n",
    "def train_speck_distinguisher(model, num_epochs, num_rounds, X_train, Y_train, X_eval, Y_eval):\n",
    "    \n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights= True);\n",
    "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size= bs, epochs=num_epochs,validation_data=(X_eval, Y_eval), callbacks=[lr, stop_early])\n",
    "    \n",
    "    return model;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the number of rounds \n",
    "num_rounds=5;\n",
    "\n",
    "#Generate training and test data - ciphertext pairs \n",
    "X_train, Y_train = make_train_data(10**7,num_rounds);\n",
    "X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
    "    \n",
    "#Construct and train the reduced network\n",
    "model = model_builder();\n",
    "trained_model = train_speck_distinguisher(model, 30, num_rounds, X_train, Y_train, X_eval, Y_eval);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class 1 = fixed difference\n",
    "#Class 0 = random difference\n",
    "\n",
    "#If prediction > 0.5 -> assigned to class 1\n",
    "#Else -> assigned to class 0 \n",
    "\n",
    "X, Y = make_train_data(1, num_rounds)\n",
    "Y_pred = trained_model.predict(X)\n",
    "\n",
    "print(\"True class: \"+str(Y[0]) +\"\\t\\t\"+ \"Prediction: \"+ str(Y_pred[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat: Train the model and evaluate the  results #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQnbavoe1RHZ"
   },
   "outputs": [],
   "source": [
    "def repeat_experiment(num_reps, num_rounds):\n",
    "\n",
    "  #Store the accs, tprs, tnrs for all experimenent repetitons\n",
    "  accs= [];\n",
    "  tprs =[];\n",
    "  tnrs=[];\n",
    "    \n",
    "  #Repeat the experiment multiple times\n",
    "  for i in range(0, num_reps):\n",
    "    \n",
    "    \n",
    "    X_train, Y_train = make_train_data(10**7,num_rounds);\n",
    "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
    "    \n",
    "    model = model_builder();\n",
    "    trained_model = train_speck_distinguisher(model, 30, num_rounds, X_train, Y_train, X_eval, Y_eval);\n",
    "    \n",
    "    (acc, tpr, tnr) = multiple_evaluations(trained_model, 5, num_rounds);\n",
    "    accs.append(acc);\n",
    "    tprs.append(tpr);\n",
    "    tnrs.append(tnr);\n",
    "\n",
    "  print(\"Round: \"+str(num_rounds));\n",
    "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
    "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
    "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
    "  \n",
    "  return (accs, tprs, tnrs);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the experiment #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_rounds = 5, 6, 7, 8;\n",
    "#num_reps = 5;\n",
    "\n",
    "repeat_experiment(num_reps= 5, num_rounds= 5);"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TheSpeckAttack_Depth-1_No_shortcut_distinguisher.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
