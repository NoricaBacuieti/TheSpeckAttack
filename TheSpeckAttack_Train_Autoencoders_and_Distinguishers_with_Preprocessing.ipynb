{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUSOSK3ZC3rL"
      },
      "outputs": [],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYrc0_7kKfpr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Conv1D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, Concatenate,Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, UpSampling1D\n",
        "from keras.regularizers import l2, l1, l1_l2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuHgGAdjqwFp"
      },
      "source": [
        "# The Speck cipher and data generation algorithms #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/speck.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-As2P_AK_gEW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from os import urandom\n",
        "\n",
        "def WORD_SIZE():\n",
        "    return(16);\n",
        "\n",
        "def ALPHA():\n",
        "    return(7);\n",
        "\n",
        "def BETA():\n",
        "    return(2);\n",
        "\n",
        "MASK_VAL = 2 ** WORD_SIZE() - 1;\n",
        "\n",
        "def shuffle_together(l):\n",
        "    state = np.random.get_state();\n",
        "    for x in l:\n",
        "        np.random.set_state(state);\n",
        "        np.random.shuffle(x);\n",
        "\n",
        "def rol(x,k):\n",
        "    return(((x << k) & MASK_VAL) | (x >> (WORD_SIZE() - k)));\n",
        "\n",
        "def ror(x,k):\n",
        "    return((x >> k) | ((x << (WORD_SIZE() - k)) & MASK_VAL));\n",
        "\n",
        "def enc_one_round(p, k):\n",
        "    c0, c1 = p[0], p[1];\n",
        "    c0 = ror(c0, ALPHA());\n",
        "    c0 = (c0 + c1) & MASK_VAL;\n",
        "    c0 = c0 ^ k;\n",
        "    c1 = rol(c1, BETA());\n",
        "    c1 = c1 ^ c0;\n",
        "    return(c0,c1);\n",
        "\n",
        "def dec_one_round(c,k):\n",
        "    c0, c1 = c[0], c[1];\n",
        "    c1 = c1 ^ c0;\n",
        "    c1 = ror(c1, BETA());\n",
        "    c0 = c0 ^ k;\n",
        "    c0 = (c0 - c1) & MASK_VAL;\n",
        "    c0 = rol(c0, ALPHA());\n",
        "    return(c0, c1);\n",
        "\n",
        "def expand_key(k, t):\n",
        "    ks = [0 for i in range(t)];\n",
        "    ks[0] = k[len(k)-1];\n",
        "    l = list(reversed(k[:len(k)-1]));\n",
        "    for i in range(t-1):\n",
        "        l[i%3], ks[i+1] = enc_one_round((l[i%3], ks[i]), i);\n",
        "    return(ks);\n",
        "\n",
        "def encrypt(p, ks):\n",
        "    x, y = p[0], p[1];\n",
        "    for k in ks:\n",
        "        x,y = enc_one_round((x,y), k);\n",
        "    return(x, y);\n",
        "\n",
        "def decrypt(c, ks):\n",
        "    x, y = c[0], c[1];\n",
        "    for k in reversed(ks):\n",
        "        x, y = dec_one_round((x,y), k);\n",
        "    return(x,y);\n",
        "\n",
        "def check_testvector():\n",
        "  key = (0x1918,0x1110,0x0908,0x0100)\n",
        "  pt = (0x6574, 0x694c)\n",
        "  ks = expand_key(key, 22)\n",
        "  ct = encrypt(pt, ks)\n",
        "  if (ct == (0xa868, 0x42f2)):\n",
        "    print(\"Testvector verified.\")\n",
        "    return(True);\n",
        "  else:\n",
        "    print(\"Testvector not verified.\")\n",
        "    return(False);\n",
        "\n",
        "#convert_to_binary takes as input an array of ciphertext pairs\n",
        "#where the first row of the array contains the lefthand side of the ciphertexts,\n",
        "#the second row contains the righthand side of the ciphertexts,\n",
        "#the third row contains the lefthand side of the second ciphertexts,\n",
        "#and so on\n",
        "#it returns an array of bit vectors containing the same data\n",
        "def convert_to_binary(arr):\n",
        "  X = np.zeros((4 * WORD_SIZE(),len(arr[0])),dtype=np.uint8);\n",
        "  for i in range(4 * WORD_SIZE()):\n",
        "    index = i // WORD_SIZE();\n",
        "    offset = WORD_SIZE() - (i % WORD_SIZE()) - 1;\n",
        "    X[i] = (arr[index] >> offset) & 1;\n",
        "  X = X.transpose();\n",
        "  return(X);\n",
        "\n",
        "#takes a text file that contains encrypted block0, block1, true diff prob, real or random\n",
        "#data samples are line separated, the above items whitespace-separated\n",
        "#returns train data, ground truth, optimal ddt prediction\n",
        "def readcsv(datei):\n",
        "    data = np.genfromtxt(datei, delimiter=' ', converters={x: lambda s: int(s,16) for x in range(2)});\n",
        "    X0 = [data[i][0] for i in range(len(data))];\n",
        "    X1 = [data[i][1] for i in range(len(data))];\n",
        "    Y = [data[i][3] for i in range(len(data))];\n",
        "    Z = [data[i][2] for i in range(len(data))];\n",
        "    ct0a = [X0[i] >> 16 for i in range(len(data))];\n",
        "    ct1a = [X0[i] & MASK_VAL for i in range(len(data))];\n",
        "    ct0b = [X1[i] >> 16 for i in range(len(data))];\n",
        "    ct1b = [X1[i] & MASK_VAL for i in range(len(data))];\n",
        "    ct0a = np.array(ct0a, dtype=np.uint16); ct1a = np.array(ct1a,dtype=np.uint16);\n",
        "    ct0b = np.array(ct0b, dtype=np.uint16); ct1b = np.array(ct1b, dtype=np.uint16);\n",
        "    \n",
        "    #X = [[X0[i] >> 16, X0[i] & 0xffff, X1[i] >> 16, X1[i] & 0xffff] for i in range(len(data))];\n",
        "    X = convert_to_binary([ct0a, ct1a, ct0b, ct1b]); \n",
        "    Y = np.array(Y, dtype=np.uint8); Z = np.array(Z);\n",
        "    return(X,Y,Z);\n",
        "\n",
        "#baseline training data generator\n",
        "def make_train_data(n, nr, diff=(0x0040,0)):\n",
        "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
        "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
        "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
        "  num_rand_samples = np.sum(Y==0);\n",
        "  plain1l[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  plain1r[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  ks = expand_key(keys, nr);\n",
        "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
        "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
        "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
        "  return(X,Y);\n",
        "\n",
        "#real differences data generator\n",
        "def real_differences_data(n, nr, diff=(0x0040,0)):\n",
        "  #generate labels\n",
        "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
        "  #generate keys\n",
        "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
        "  #generate plaintexts\n",
        "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  #apply input difference\n",
        "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
        "  num_rand_samples = np.sum(Y==0);\n",
        "  #expand keys and encrypt\n",
        "  ks = expand_key(keys, nr);\n",
        "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
        "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
        "  #generate blinding values\n",
        "  k0 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  k1 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  #apply blinding to the samples labelled as random\n",
        "  ctdata0l[Y==0] = ctdata0l[Y==0] ^ k0; ctdata0r[Y==0] = ctdata0r[Y==0] ^ k1;\n",
        "  ctdata1l[Y==0] = ctdata1l[Y==0] ^ k0; ctdata1r[Y==0] = ctdata1r[Y==0] ^ k1;\n",
        "  #convert to input data for neural networks\n",
        "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
        "  return(X,Y);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFRQpbmPq2Gd"
      },
      "source": [
        "# Evaluate the results #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/eval.py and slightly adapted for evaluating the results of an autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JUEUGNtKmM_"
      },
      "outputs": [],
      "source": [
        "def evaluate(net,X,Y):\n",
        "    \n",
        "    Z = net.predict(X,batch_size=5000).flatten();\n",
        "    Zbin = (Z > 0.5);\n",
        "\n",
        "    #Compute the acc, tpr, tnr\n",
        "    n = len(Z); \n",
        "    n0 = np.sum(Y==0); \n",
        "    n1 = np.sum(Y==1);\n",
        "    \n",
        "    acc = np.sum(Zbin == Y.flatten()) / n;\n",
        "    tpr = np.sum(Zbin[Y.flatten()==1]) / n1;\n",
        "    tnr = np.sum(Zbin[Y.flatten()==0] == 0) / n0;\n",
        "\n",
        "    return(acc, tpr, tnr);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWNF6zOVq-tP"
      },
      "source": [
        "# Conduct multiple evaluations #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOxHHI6UKnpf"
      },
      "outputs": [],
      "source": [
        "def multiple_evaluations(model, repetitions, num_rounds):\n",
        " \n",
        "  #The accs, tprs, tnrs for all evaluation repetition\n",
        "  accs = [];\n",
        "  tprs = [];\n",
        "  tnrs = [];\n",
        "    \n",
        "  #Evaluate multiple times and average the results \n",
        "  for i in range(0, repetitions):\n",
        "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "\n",
        "    (acc, tpr, tnr) = evaluate(model, X_eval, X_eval);\n",
        "    accs.append(acc);\n",
        "    tprs.append(tpr);\n",
        "    tnrs.append(tnr);\n",
        "\n",
        "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
        "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
        "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
        "        \n",
        "  return(np.mean(accs), np.mean(tprs), np.mean(tnrs));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "327WPvIgKfpy"
      },
      "source": [
        "# Building the Autoencoder #\n",
        "The implementation for all three versions is provided below (already unfolded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACV-2KFWiQ8n"
      },
      "outputs": [],
      "source": [
        "#The encoder\n",
        "def encode(input):\n",
        "\n",
        "  x = Reshape((4, 16))(input);\n",
        "  x = Permute((2,1))(x);\n",
        "\n",
        "  #Block 1\n",
        "  x = Conv1D(32, 3, padding='same')(x)\n",
        "  x = BatchNormalization()(x);\n",
        "  x = Activation('relu')(x);\n",
        "  x = MaxPooling1D(2, padding='same')(x)\n",
        "\n",
        "  #Block 2\n",
        "  #x = Conv1D(32, 3, padding='same')(x)\n",
        "  #x = BatchNormalization()(x);\n",
        "  #x = Activation('relu')(x);\n",
        "  #x = MaxPooling1D(2, padding='same')(x)\n",
        "    \n",
        "  #Block 3  \n",
        "  #x = Conv1D(32, 3, padding='same')(x)\n",
        "  #x = BatchNormalization()(x);\n",
        "  #x = Activation('relu')(x);\n",
        "  #x = MaxPooling1D(2, padding='same')(x)\n",
        "\n",
        "  encoded = x;\n",
        "    \n",
        "  return encoded;\n",
        "\n",
        "\n",
        "#The decoder\n",
        "def decode(encoded):\n",
        "  \n",
        "  #Block 1 \n",
        "  x = Conv1D(32, 3, padding='same')(encoded)\n",
        "  x = BatchNormalization()(x);\n",
        "  x = Activation('relu')(x);\n",
        "  x = UpSampling1D(2)(x)\n",
        "\n",
        "  #Block 2  \n",
        "  #x = Conv1D(32, 3, padding='same')(x)\n",
        "  #x = BatchNormalization()(x);\n",
        "  #x = Activation('relu')(x);\n",
        "  #x = UpSampling1D(2)(x)\n",
        "\n",
        "  #Block 3\n",
        "  #x = Conv1D(32, 3, padding='same')(x)\n",
        "  #x = BatchNormalization()(x);\n",
        "  #x = Activation('relu')(x);\n",
        "  #x = UpSampling1D(2)(x)\n",
        "\n",
        " \n",
        "  x = Conv1D(4, 3, activation='sigmoid', padding='same')(x)\n",
        "  x = Permute((2,1))(x);\n",
        "  decoded = Reshape((64,1))(x);\n",
        "\n",
        "  return decoded;\n",
        "\n",
        "\n",
        "#Using both the encoder and decoder to construct the autoencoder\n",
        "def build_autoencoder():\n",
        "    \n",
        "  inp = Input(shape=(64,));\n",
        "\n",
        "  encoded= encode(inp);\n",
        "  decoded = decode(encoded);\n",
        "\n",
        "  autoencoder = Model(inp, decoded);\n",
        "\n",
        "  autoencoder.compile(\n",
        "          optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc'])\n",
        "\n",
        "  return autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnsKYtOcrXOJ"
      },
      "source": [
        "For training the autoencoder \\\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/train_nets.py and slightly adapted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTgp2ALnmHfr"
      },
      "outputs": [],
      "source": [
        "#Batch size\n",
        "bs=5000;\n",
        "\n",
        "def cyclic_lr(num_epochs, high_lr, low_lr):\n",
        "  res = lambda i: low_lr + ((num_epochs-1) - i % num_epochs)/(num_epochs-1) * (high_lr - low_lr);\n",
        "  return(res);\n",
        "\n",
        "def train_autoenc(model, num_epochs, num_rounds, X_train, Y_train, X_eval, Y_eval):\n",
        "    \n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights= True);\n",
        "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
        "    \n",
        "    model.fit(X_train, Y_train, batch_size= bs, epochs=num_epochs,validation_data=(X_eval, Y_eval), callbacks=[lr, stop_early])\n",
        "    \n",
        "    return model;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkgGXWtpKfp0"
      },
      "source": [
        "# Repeat: Train the autoencoder and evaluate the results #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRAXF9ar361h"
      },
      "outputs": [],
      "source": [
        "def repeat_experiment(repetitions, num_rounds):\n",
        " \n",
        "  #Store the accs, tprs, tnrs for all experimenent repetitons\n",
        "  accs = [];\n",
        "  tprs = [];\n",
        "  tnrs = [];\n",
        "    \n",
        "  #Repeat the experiment multiple times\n",
        "  for i in range(0, repetitions):\n",
        "    \n",
        "    X_train, Y_train = make_train_data(10**7, num_rounds);\n",
        "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "    \n",
        "    initial_model = build_autoencoder();\n",
        "    #Set Y_train to X_train and Y_eval to X_eval\n",
        "    trained_model= train_autoenc(initial_model, 30, num_rounds, X_train, X_train, X_eval, X_eval);\n",
        "\n",
        "    (acc, tpr, tnr) = multiple_evaluations(trained_model,5,num_rounds);\n",
        "    accs.append(acc);\n",
        "    tprs.append(tpr);\n",
        "    tnrs.append(tnr);\n",
        "\n",
        "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
        "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
        "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
        "        \n",
        "  return(np.mean(accs), np.mean(tprs), np.mean(tnrs));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlt9OhkFC3rw"
      },
      "source": [
        "# Run the experiment #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbsCeFZ6_HPr"
      },
      "outputs": [],
      "source": [
        "#num_rounds = 5, 6, 7, 8 with an autoencoder with 1/2/3 blocks (need to un/comment the blocks in the autoencoder implementation - un/comment both the block from the encoder and its conterpart from the decoder so that the ecoder and decoder will have the same number of blocks)\n",
        "#num_repetitions = 5\n",
        "\n",
        "repeat_experiment(repetitions = 5, num_rounds =5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8sfmHrXC3rx"
      },
      "source": [
        " ----------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kl-osRns7-y"
      },
      "source": [
        "# Steps for training with preprocessed input  #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtxa38OO_HPw"
      },
      "source": [
        "# 1. Train the autoencoder of choice # "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxN5Uy_y_HPw"
      },
      "outputs": [],
      "source": [
        "#Specify the number of rounds for which the autoencoder should be trained\n",
        "num_rounds = 5;\n",
        "\n",
        "X_train, Y_train = make_train_data(10**7, num_rounds);\n",
        "X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "    \n",
        "initial_model = build_autoencoder();\n",
        "\n",
        "#Set Y_train to X_train and Y_eval to X_eval\n",
        "trained_model= train_autoenc(initial_model, 30, num_rounds, X_train, X_train, X_eval, X_eval);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMSCBV-wC3r0"
      },
      "source": [
        "Let's look at a predicition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hM6ZceNC3r0"
      },
      "outputs": [],
      "source": [
        "#Generate an instance and predict \n",
        "X, _ = make_train_data(1, num_rounds)\n",
        "X_pred = trained_model.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvicsUm5C3r3"
      },
      "outputs": [],
      "source": [
        "#Print the result\n",
        "\n",
        "X_print= X[0];\n",
        "X_pred_print = list(X_pred[0].flatten());\n",
        "\n",
        "for i in range(64):\n",
        "  print(\"At pozition: \"+str(i) +\"\\t\\t\"+ \"True value: \" +str(X_print[i])+\"\\t\\t\" + \"Predicted value: \"+str(X_pred_print[i]));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGyIf-waKfp1"
      },
      "source": [
        "# 2. Create an encoder model and set its weights to those of the encoder trained within the autoencoder from above #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb-dPP5bR6tp"
      },
      "outputs": [],
      "source": [
        "#Create the encoder model\n",
        "inpe = Input(shape=(64,));\n",
        "oute = encode(inpe); \n",
        "encoder_model = Model(inpe,oute)\n",
        "\n",
        "encoder_model.compile(\n",
        "          optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc']);\n",
        "\n",
        "#Set the weights to those of the pretrained encoder (from within the above-trained autoencoder)\n",
        "for l1,l2 in zip(encoder_model.layers,trained_model.layers):\n",
        "    l1.set_weights(l2.get_weights())\n",
        "    print(l1.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQjl7FN6Kfp2"
      },
      "source": [
        "# 3. Do not allow the weights of the encoder to change #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coBFk-aUKfp2"
      },
      "outputs": [],
      "source": [
        "for layer in encoder_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2L9OuWo_HP0"
      },
      "source": [
        "# 4. Add the preprocessing step to the (reduced/depth-1/10) distinguisher #\n",
        "#### The network of the reduced distinguisher is given below.  \n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/train_nets.py and slightly adapted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCTV3H78_HP1"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Conv1D,Conv2D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, Concatenate,Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, UpSampling1D\n",
        "from keras.regularizers import l2, l1, l1_l2\n",
        "\n",
        "\n",
        "def cyclic_lr(num_epochs, high_lr, low_lr):\n",
        "  res = lambda i: low_lr + ((num_epochs-1) - i % num_epochs)/(num_epochs-1) * (high_lr - low_lr);\n",
        "  return(res);\n",
        "\n",
        "#Batch size\n",
        "bs = 5000;\n",
        "\n",
        "\n",
        "def make_resnet( num_blocks=2, num_filters=32, num_outputs=1, d1=64, d2=64, word_size=16, ks=3,depth=5, reg_param=0.0001, final_activation='sigmoid'):\n",
        "\n",
        "  #Input and preprocessing layers\n",
        "  inp = Input(shape=(64,));\n",
        "\n",
        "  #Encode the input  \n",
        "  x = encoder_model.call(inp); \n",
        "\n",
        "  \n",
        "  #Please paste your network of choice within the dotted lines - here, the discovered reduced network is given\n",
        "  #Also, do not forget to set the input of your network to x (the encoded inputs)  \n",
        "  \n",
        "  #---------------------------------------------------\n",
        "  conv0 = Conv1D(25, kernel_size=1, padding='same')(x);\n",
        "  conv0 = BatchNormalization()(conv0);\n",
        "  conv0 = Activation('relu')(conv0);\n",
        "\n",
        "  conv1 = Conv1D(11, kernel_size=ks, padding='same')(conv0);\n",
        "  conv1 = BatchNormalization()(conv1);\n",
        "  conv1 = Activation('relu')(conv1);\n",
        "\n",
        "  conv2 = Conv1D(7, kernel_size=ks, padding='same')(conv1);\n",
        "  conv2 = BatchNormalization()(conv2);\n",
        "  conv2 = Activation('relu')(conv2);\n",
        "\n",
        "  flat = Flatten()(conv2);\n",
        "\n",
        "  dense1 = Dense(18)(flat);\n",
        "  dense1 = BatchNormalization()(dense1);\n",
        "  dense1 = Activation('relu')(dense1);\n",
        "    \n",
        "  dense2 = Dense(28)(dense1);\n",
        "  dense2 = BatchNormalization()(dense2);\n",
        "  dense2 = Activation('relu')(dense2);\n",
        "\n",
        "  out = Dense(1, activation='sigmoid')(dense2);\n",
        "  #---------------------------------------------------\n",
        "    \n",
        "    \n",
        "  model = Model(inputs=inp, outputs=out);\n",
        "\n",
        "  return model;\n",
        "\n",
        "\n",
        "\n",
        "def model_builder(depth):\n",
        "\n",
        "  model = make_resnet(depth=depth);\n",
        "\n",
        "  model.compile(\n",
        "          optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc']);\n",
        "    \n",
        "  return model;\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def train_speck_distinguisher(model, num_epochs, num_rounds, X_train, Y_train, X_eval, Y_eval):\n",
        "    \n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights= True);\n",
        "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
        "    \n",
        "    model.fit(X_train, Y_train, batch_size= bs, epochs=num_epochs, validation_data=(X_eval, Y_eval), callbacks=[lr, stop_early])\n",
        "    \n",
        "    return model;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7QJ5wRAC3r8"
      },
      "source": [
        "# 5. Run the experiment #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQYhtpxT_HP1"
      },
      "outputs": [],
      "source": [
        "#You can take the network (without the preprocessing of the inputs) directly from https://github.com/agohr/deep_speck/blob/master/train_nets.py \n",
        "#Change the depth to 10 or 1 to conduct the other experiments\n",
        "#Or take the network from the end of this notebook\n",
        "\n",
        "X_train, Y_train = make_train_data(10**7, num_rounds);\n",
        "X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "\n",
        "initial_modele = model_builder(depth=1); \n",
        "trained_modele= train_speck_distinguisher(initial_modele, 30, num_rounds, X_train, Y_train, X_eval, Y_eval);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84va3ssuC3r9"
      },
      "source": [
        "The evaluate function for evaluating the model with the encoder as a preprocessor.\\\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/eval.py and slightly adapted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r17KtShwC3r-"
      },
      "source": [
        "Evaluate the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7UbESw5C3r-"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_with_preprocessing(net,X,Y):\n",
        "    \n",
        "    Z = net.predict(X,batch_size=10000).flatten();\n",
        "    Zbin = (Z > 0.5);\n",
        "    \n",
        "    #Compute the acc, tpr, tnr\n",
        "    n = len(Z); \n",
        "    n0 = np.sum(Y==0); \n",
        "    n1 = np.sum(Y==1);\n",
        "    \n",
        "    acc = np.sum(Zbin == Y) / n;\n",
        "    tpr = np.sum(Zbin[Y==1]) / n1;\n",
        "    tnr = np.sum(Zbin[Y==0] == 0) / n0;\n",
        "    \n",
        "    return(acc, tpr, tnr); "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bOPm6HSC3r_"
      },
      "source": [
        "Conduct multiple evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j8tt-x5C3r_"
      },
      "outputs": [],
      "source": [
        "def multiple_evaluations_model_with_preprocessing(model, repetitions, num_rounds):\n",
        " \n",
        "  #The accs, tprs, tnrs for all evaluation repetition\n",
        "  accs = [];\n",
        "  tprs = [];\n",
        "  tnrs = [];\n",
        "    \n",
        "  #Evaluate multiple times and average the results \n",
        "  for i in range(0, repetitions):\n",
        "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "\n",
        "    (acc, tpr, tnr) = evaluate_model_with_preprocessing(model, X_eval, Y_eval);\n",
        "    accs.append(acc);\n",
        "    tprs.append(tpr);\n",
        "    tnrs.append(tnr);\n",
        "\n",
        "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
        "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
        "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
        "        \n",
        "  return(np.mean(accs), np.mean(tprs), np.mean(tnrs));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66D0tvZTC3sA"
      },
      "source": [
        "Gohr's network for conducting the above experiment with the depth-1/10 distinguisher \\\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/train_nets.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E5qR64TC3sA"
      },
      "outputs": [],
      "source": [
        "  #Block 1\n",
        "  conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(x);\n",
        "  conv0 = BatchNormalization()(conv0);\n",
        "  conv0 = Activation('relu')(conv0);\n",
        "\n",
        "  #Blocks 2-i - residual blocks\n",
        "  shortcut = conv0;\n",
        "  for i in range(depth):\n",
        "    conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut);\n",
        "    conv1 = BatchNormalization()(conv1);\n",
        "    conv1 = Activation('relu')(conv1);\n",
        "    \n",
        "    conv2 = Conv1D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1);\n",
        "    conv2 = BatchNormalization()(conv2);\n",
        "    conv2 = Activation('relu')(conv2);\n",
        "    shortcut = Add()([shortcut, conv2]);\n",
        "    \n",
        "  #Block 3\n",
        "  flat1 = Flatten()(shortcut);\n",
        "    \n",
        "  dense1 = Dense(d1,kernel_regularizer=l2(reg_param))(flat1);\n",
        "  dense1 = BatchNormalization()(dense1);\n",
        "  dense1 = Activation('relu')(dense1);\n",
        "\n",
        "  dense2 = Dense(d2, kernel_regularizer=l2(reg_param))(dense1);\n",
        "  dense2 = BatchNormalization()(dense2);\n",
        "  dense2 = Activation('relu')(dense2);\n",
        "    \n",
        "  out = Dense(num_outputs, activation=final_activation, kernel_regularizer=l2(reg_param))(dense2);"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TheSpeckAttack_Train_Autoencoders_and_Distinguishers_with_Preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}