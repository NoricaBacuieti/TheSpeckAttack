{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TheSpeckAttack_Train_Autoencoders_and_Distinguishers_with_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypPZEY9BuckP"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYrc0_7kKfpr"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Conv1D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, Concatenate,Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, UpSampling1D\n",
        "from keras.regularizers import l2, l1, l1_l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuHgGAdjqwFp"
      },
      "source": [
        "# The Speck cipher and data generation algorithms #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/speck.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-As2P_AK_gEW"
      },
      "source": [
        "import numpy as np\n",
        "from os import urandom\n",
        "\n",
        "def WORD_SIZE():\n",
        "    return(16);\n",
        "\n",
        "def ALPHA():\n",
        "    return(7);\n",
        "\n",
        "def BETA():\n",
        "    return(2);\n",
        "\n",
        "MASK_VAL = 2 ** WORD_SIZE() - 1;\n",
        "\n",
        "def shuffle_together(l):\n",
        "    state = np.random.get_state();\n",
        "    for x in l:\n",
        "        np.random.set_state(state);\n",
        "        np.random.shuffle(x);\n",
        "\n",
        "def rol(x,k):\n",
        "    return(((x << k) & MASK_VAL) | (x >> (WORD_SIZE() - k)));\n",
        "\n",
        "def ror(x,k):\n",
        "    return((x >> k) | ((x << (WORD_SIZE() - k)) & MASK_VAL));\n",
        "\n",
        "def enc_one_round(p, k):\n",
        "    c0, c1 = p[0], p[1];\n",
        "    c0 = ror(c0, ALPHA());\n",
        "    c0 = (c0 + c1) & MASK_VAL;\n",
        "    c0 = c0 ^ k;\n",
        "    c1 = rol(c1, BETA());\n",
        "    c1 = c1 ^ c0;\n",
        "    return(c0,c1);\n",
        "\n",
        "def dec_one_round(c,k):\n",
        "    c0, c1 = c[0], c[1];\n",
        "    c1 = c1 ^ c0;\n",
        "    c1 = ror(c1, BETA());\n",
        "    c0 = c0 ^ k;\n",
        "    c0 = (c0 - c1) & MASK_VAL;\n",
        "    c0 = rol(c0, ALPHA());\n",
        "    return(c0, c1);\n",
        "\n",
        "def expand_key(k, t):\n",
        "    ks = [0 for i in range(t)];\n",
        "    ks[0] = k[len(k)-1];\n",
        "    l = list(reversed(k[:len(k)-1]));\n",
        "    for i in range(t-1):\n",
        "        l[i%3], ks[i+1] = enc_one_round((l[i%3], ks[i]), i);\n",
        "    return(ks);\n",
        "\n",
        "def encrypt(p, ks):\n",
        "    x, y = p[0], p[1];\n",
        "    for k in ks:\n",
        "        x,y = enc_one_round((x,y), k);\n",
        "    return(x, y);\n",
        "\n",
        "def decrypt(c, ks):\n",
        "    x, y = c[0], c[1];\n",
        "    for k in reversed(ks):\n",
        "        x, y = dec_one_round((x,y), k);\n",
        "    return(x,y);\n",
        "\n",
        "def check_testvector():\n",
        "  key = (0x1918,0x1110,0x0908,0x0100)\n",
        "  pt = (0x6574, 0x694c)\n",
        "  ks = expand_key(key, 22)\n",
        "  ct = encrypt(pt, ks)\n",
        "  if (ct == (0xa868, 0x42f2)):\n",
        "    print(\"Testvector verified.\")\n",
        "    return(True);\n",
        "  else:\n",
        "    print(\"Testvector not verified.\")\n",
        "    return(False);\n",
        "\n",
        "#convert_to_binary takes as input an array of ciphertext pairs\n",
        "#where the first row of the array contains the lefthand side of the ciphertexts,\n",
        "#the second row contains the righthand side of the ciphertexts,\n",
        "#the third row contains the lefthand side of the second ciphertexts,\n",
        "#and so on\n",
        "#it returns an array of bit vectors containing the same data\n",
        "def convert_to_binary(arr):\n",
        "  X = np.zeros((4 * WORD_SIZE(),len(arr[0])),dtype=np.uint8);\n",
        "  for i in range(4 * WORD_SIZE()):\n",
        "    index = i // WORD_SIZE();\n",
        "    offset = WORD_SIZE() - (i % WORD_SIZE()) - 1;\n",
        "    X[i] = (arr[index] >> offset) & 1;\n",
        "  X = X.transpose();\n",
        "  return(X);\n",
        "\n",
        "#takes a text file that contains encrypted block0, block1, true diff prob, real or random\n",
        "#data samples are line separated, the above items whitespace-separated\n",
        "#returns train data, ground truth, optimal ddt prediction\n",
        "def readcsv(datei):\n",
        "    data = np.genfromtxt(datei, delimiter=' ', converters={x: lambda s: int(s,16) for x in range(2)});\n",
        "    X0 = [data[i][0] for i in range(len(data))];\n",
        "    X1 = [data[i][1] for i in range(len(data))];\n",
        "    Y = [data[i][3] for i in range(len(data))];\n",
        "    Z = [data[i][2] for i in range(len(data))];\n",
        "    ct0a = [X0[i] >> 16 for i in range(len(data))];\n",
        "    ct1a = [X0[i] & MASK_VAL for i in range(len(data))];\n",
        "    ct0b = [X1[i] >> 16 for i in range(len(data))];\n",
        "    ct1b = [X1[i] & MASK_VAL for i in range(len(data))];\n",
        "    ct0a = np.array(ct0a, dtype=np.uint16); ct1a = np.array(ct1a,dtype=np.uint16);\n",
        "    ct0b = np.array(ct0b, dtype=np.uint16); ct1b = np.array(ct1b, dtype=np.uint16);\n",
        "    \n",
        "    #X = [[X0[i] >> 16, X0[i] & 0xffff, X1[i] >> 16, X1[i] & 0xffff] for i in range(len(data))];\n",
        "    X = convert_to_binary([ct0a, ct1a, ct0b, ct1b]); \n",
        "    Y = np.array(Y, dtype=np.uint8); Z = np.array(Z);\n",
        "    return(X,Y,Z);\n",
        "\n",
        "#baseline training data generator\n",
        "def make_train_data(n, nr, diff=(0x0040,0)):\n",
        "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
        "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
        "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
        "  num_rand_samples = np.sum(Y==0);\n",
        "  plain1l[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  plain1r[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  ks = expand_key(keys, nr);\n",
        "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
        "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
        "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
        "  return(X,Y);\n",
        "\n",
        "#real differences data generator\n",
        "def real_differences_data(n, nr, diff=(0x0040,0)):\n",
        "  #generate labels\n",
        "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
        "  #generate keys\n",
        "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
        "  #generate plaintexts\n",
        "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
        "  #apply input difference\n",
        "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
        "  num_rand_samples = np.sum(Y==0);\n",
        "  #expand keys and encrypt\n",
        "  ks = expand_key(keys, nr);\n",
        "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
        "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
        "  #generate blinding values\n",
        "  k0 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  k1 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
        "  #apply blinding to the samples labelled as random\n",
        "  ctdata0l[Y==0] = ctdata0l[Y==0] ^ k0; ctdata0r[Y==0] = ctdata0r[Y==0] ^ k1;\n",
        "  ctdata1l[Y==0] = ctdata1l[Y==0] ^ k0; ctdata1r[Y==0] = ctdata1r[Y==0] ^ k1;\n",
        "  #convert to input data for neural networks\n",
        "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
        "  return(X,Y);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFRQpbmPq2Gd"
      },
      "source": [
        "# Evaluate the results #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/eval.py and slightly adapted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JUEUGNtKmM_"
      },
      "source": [
        "def evaluate(net,X,Y):\n",
        "    Z = net.predict(X,batch_size=5000).flatten();\n",
        "    Zbin = (Z > 0.5);\n",
        "    diff = Y.flatten() - Z; \n",
        "    mse = np.mean(diff*diff);\n",
        "    n = len(Z); n0 = np.sum(Y==0); n1 = np.sum(Y==1);\n",
        "    acc = np.sum(Zbin == Y.flatten()) / n;\n",
        "    tpr = np.sum(Zbin[Y.flatten()==1]) / n1;\n",
        "    tnr = np.sum(Zbin[Y.flatten()==0] == 0) / n0;\n",
        "    mreal = np.median(Z[Y.flatten()==1]);\n",
        "    high_random = np.sum(Z[Y.flatten()==0] > mreal) / n0;\n",
        "\n",
        "    return(acc, tpr, tnr); # added\n",
        "    #print(\"Accuracy: \", acc, \"TPR: \", tpr, \"TNR: \", tnr, \"MSE:\", mse);\n",
        "    #print(\"Percentage of random pairs with score higher than median of real pairs:\", 100*high_random);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWNF6zOVq-tP"
      },
      "source": [
        "# Conducting multiple evaluations #\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOxHHI6UKnpf"
      },
      "source": [
        "def multiple_evaluations(model, repetitions, num_rounds):\n",
        " \n",
        "  accs = [];\n",
        "  tprs = [];\n",
        "  tnrs = [];\n",
        "  for i in range(0, repetitions):\n",
        "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "    (acc, tpr, tnr) = evaluate(model, X_eval, X_eval);\n",
        "    accs.append(acc);\n",
        "    tprs.append(tpr);\n",
        "    tnrs.append(tnr);\n",
        "\n",
        "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
        "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
        "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
        "        \n",
        "  return(np.mean(accs), np.mean(tprs), np.mean(tnrs));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "327WPvIgKfpy"
      },
      "source": [
        "# Building the Autoencoder #\n",
        "The implementation for all three layers is provided below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACV-2KFWiQ8n"
      },
      "source": [
        "\n",
        "def encode(input):\n",
        "\n",
        "  x = Reshape((4, 16))(input);\n",
        "  x = Permute((2,1))(x);\n",
        "\n",
        "  #Layer 1\n",
        "  x = Conv1D(32, 3, padding='same')(x)\n",
        "  x = BatchNormalization()(x);\n",
        "  x = Activation('relu')(x);\n",
        "  x = MaxPooling1D(2, padding='same')(x)\n",
        "\n",
        "  #Layer 2\n",
        "  #x = Conv1D(32, 3, padding='same')(x)\n",
        "  #x = BatchNormalization()(x);\n",
        "  #x = Activation('relu')(x);\n",
        "  #x = MaxPooling1D(2, padding='same')(x)\n",
        "    \n",
        "  #Layer 3  \n",
        "  #x = Conv1D(32, 3, padding='same')(x)\n",
        "  #x = BatchNormalization()(x);\n",
        "  #x = Activation('relu')(x);\n",
        "  #x = MaxPooling1D(2, padding='same')(x)\n",
        "\n",
        "  encoded = x;\n",
        "    \n",
        "\n",
        "\n",
        "  return encoded;\n",
        "\n",
        "\n",
        "def decode(encoded):\n",
        "  \n",
        "  x = Conv1D(32, 3, padding='same')(encoded)\n",
        "  x = BatchNormalization()(x);\n",
        "  x = Activation('relu')(x);\n",
        "  x = UpSampling1D(2)(x)\n",
        "\n",
        "  #x = Conv1D(32, 3, padding='same')(x)\n",
        "  #x = BatchNormalization()(x);\n",
        "  #x = Activation('relu')(x);\n",
        "  #x = UpSampling1D(2)(x)\n",
        "\n",
        "  #x = Conv1D(32, 3, padding='same')(x)\n",
        "  #x = BatchNormalization()(x);\n",
        "  #x = Activation('relu')(x);\n",
        "  #x = UpSampling1D(2)(x)\n",
        "\n",
        " \n",
        "  x = Conv1D(4, 3, activation='sigmoid', padding='same')(x)\n",
        "  x = Permute((2,1))(x);\n",
        "  decoded = Reshape((64,1))(x);\n",
        "\n",
        "  return decoded;\n",
        "\n",
        "\n",
        "def build_autoencoder():\n",
        "  inp = Input(shape=(64,));\n",
        "  encoded= encode(inp);\n",
        "  decoded = decode(encoded);\n",
        "  autoencoder = Model(inp, decoded);\n",
        "\n",
        "  autoencoder.compile(\n",
        "          optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc'])\n",
        "  return autoencoder\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnsKYtOcrXOJ"
      },
      "source": [
        "# Training one model #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/train_nets.py and slightly adapted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGEYWqEjKfpz"
      },
      "source": [
        "def cyclic_lr(num_epochs, high_lr, low_lr):\n",
        "  res = lambda i: low_lr + ((num_epochs-1) - i % num_epochs)/(num_epochs-1) * (high_lr - low_lr);\n",
        "  return(res);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTgp2ALnmHfr"
      },
      "source": [
        "bs=5000;\n",
        "\n",
        "def train_autoenc(model, num_epochs, num_rounds, X_train, Y_train, X_eval, Y_eval):\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights= True);\n",
        "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
        "    history = model.fit(X_train, Y_train, batch_size= bs, epochs=num_epochs,validation_data=(X_eval, Y_eval), callbacks=[lr, stop_early])\n",
        "    return(model);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkgGXWtpKfp0"
      },
      "source": [
        "# Training the autoencoder and evaltuating the results multiple times #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRAXF9ar361h"
      },
      "source": [
        "def multiple_trials(repetitions, num_rounds):\n",
        " \n",
        "  accs = [];\n",
        "  tprs = [];\n",
        "  tnrs = [];\n",
        "  for i in range(0, repetitions):\n",
        "    X_train, Y_train = make_train_data(10**7, num_rounds);\n",
        "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "    initial_model = build_autoencoder();\n",
        "    trained_model= train_autoenc(initial_model, 30, num_rounds, X_train, X_train, X_eval, X_eval);\n",
        "\n",
        "    (acc, tpr, tnr) = multiple_evaluations(trained_model,5,num_rounds);\n",
        "    accs.append(acc);\n",
        "    tprs.append(tpr);\n",
        "    tnrs.append(tnr);\n",
        "\n",
        "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
        "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
        "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
        "        \n",
        "  return(np.mean(accs), np.mean(tprs), np.mean(tnrs));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbsCeFZ6_HPr",
        "scrolled": true
      },
      "source": [
        "multiple_trials(repetitions = 5, num_rounds =5) #num_rounds = 6, 7, 8 with an autoencoder with 1/2/3 layers (need to un/comment the lines in the autoencoder implementation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kl-osRns7-y"
      },
      "source": [
        "# Now, train with a preprocessing step done with an encoder  #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtxa38OO_HPw"
      },
      "source": [
        "# Train the autoencoder of choice # "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxN5Uy_y_HPw"
      },
      "source": [
        "num_rounds = 5;\n",
        "\n",
        "X_train, Y_train = make_train_data(10**7, num_rounds);\n",
        "X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
        "    \n",
        "initial_model = build_autoencoder();\n",
        "trained_model= train_autoenc(initial_model, 30, num_rounds, X_train, X_train, X_eval, X_eval);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGyIf-waKfp1"
      },
      "source": [
        "# Create encoder model and set its weights to the the pretrained weights from the encoder trained inside the autoencoder from above #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb-dPP5bR6tp"
      },
      "source": [
        "inpe = Input(shape=(64,));\n",
        "oute = encode(inpe); \n",
        "encoder_model = Model(inpe,oute)\n",
        "\n",
        "encoder_model.compile(\n",
        "          optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc']);\n",
        "\n",
        "for l1,l2 in zip(encoder_model.layers[:18],trained_model.layers[0:18]):\n",
        "    l1.set_weights(l2.get_weights())\n",
        "    print(l1.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQjl7FN6Kfp2"
      },
      "source": [
        "# Do not allow it the weights of the encoder to change #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coBFk-aUKfp2"
      },
      "source": [
        "for layer in encoder_model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2L9OuWo_HP0"
      },
      "source": [
        "# Add the preprocessing step to the (reduced/depth-1/10) distinguisher - Pruned version #\n",
        "Taken from https://github.com/agohr/deep_speck/blob/master/train_nets.py and slightly adapted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCTV3H78_HP1"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Conv1D,Conv2D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, Concatenate,Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, UpSampling1D\n",
        "from keras.regularizers import l2, l1, l1_l2\n",
        "\n",
        "\n",
        "def cyclic_lr(num_epochs, high_lr, low_lr):\n",
        "  res = lambda i: low_lr + ((num_epochs-1) - i % num_epochs)/(num_epochs-1) * (high_lr - low_lr);\n",
        "  return(res);\n",
        "\n",
        "def make_checkpoint_best_worst(datei):\n",
        "  res = ModelCheckpoint(datei, monitor='val_acc', save_best_only = True, save_weights_only=True );\n",
        "  return(res);\n",
        "\n",
        "def make_checkpoint_all(datei):\n",
        "  res = ModelCheckpoint(datei, monitor='val_acc', save_best_only = False, save_weights_only=True);\n",
        "  return(res);\n",
        "\n",
        "\n",
        "wdir_curent_bw = \"./current_bw/\";\n",
        "wdir_curent_bw_all= \"./current_bw_all/\";\n",
        "\n",
        "bs = 5000;\n",
        "\n",
        "def make_resnet( num_blocks=2, num_filters=32, num_outputs=1, d1=64, d2=64, word_size=16, ks=3,depth=5, reg_param=0.0001, final_activation='sigmoid'):\n",
        "  #Input and preprocessing layers\n",
        "  #Input and preprocessing layers\n",
        "\n",
        "  #input is 10^7 samples of 64 bits each\n",
        "  inp = Input(shape=(64,));\n",
        " \n",
        "  x = encoder_model.call(inp); # encode the input\n",
        "\n",
        "  \n",
        "  \n",
        "  conv0 = Conv1D(25, kernel_size=1, padding='same')(x);\n",
        "  conv0 = BatchNormalization()(conv0);\n",
        "  conv0 = Activation('relu')(conv0);\n",
        "\n",
        "  conv1 = Conv1D(11, kernel_size=ks, padding='same')(conv0);\n",
        "  conv1 = BatchNormalization()(conv1);\n",
        "  conv1 = Activation('relu')(conv1);\n",
        "\n",
        "  conv2 = Conv1D(7, kernel_size=ks, padding='same')(conv1);\n",
        "  conv2 = BatchNormalization()(conv2);\n",
        "  conv2 = Activation('relu')(conv2);\n",
        "\n",
        "  flat = Flatten()(conv2);\n",
        "\n",
        "  dense1 = Dense(18)(flat);\n",
        "  dense1 = BatchNormalization()(dense1);\n",
        "  dense1 = Activation('relu')(dense1);\n",
        "  dense2 = Dense(28)(dense1);\n",
        "  dense2 = BatchNormalization()(dense2);\n",
        "  dense2 = Activation('relu')(dense2);\n",
        "  out = Dense(1, activation='sigmoid')(dense2);\n",
        "  model = Model(inputs=inp, outputs=out);\n",
        "\n",
        "  return model\n",
        "\n",
        "def model_builder(depth):\n",
        "\n",
        "  model = make_resnet(depth=depth);\n",
        "  model.compile(\n",
        "          optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc']);\n",
        "  return model;\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def train_speck_distinguisher(model, num_epochs, num_rounds, X_train, Y_train, X_eval, Y_eval):\n",
        "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights= True);\n",
        "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
        "    history = model.fit(X_train, Y_train, batch_size= bs, epochs=num_epochs,validation_data=(X_eval, Y_eval), callbacks=[lr, stop_early])\n",
        "    \n",
        "    return(model);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQYhtpxT_HP1"
      },
      "source": [
        "initial_modele = model_builder(depth=1); #modify the number of filters/neurons back to the original values found  https://github.com/agohr/deep_speck/blob/master/train_nets.py  and the depth to 10 to conduct the other experiments\n",
        "trained_modele= train_speck_distinguisher(initial_modele, 30, num_rounds, X_train, Y_train, X_eval, Y_eval);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
