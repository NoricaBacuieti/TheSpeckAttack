{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3CIABYRvuUv"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.2.0\n",
    "!pip install keras\n",
    "!pip install kerassurgeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypPZEY9BuckP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from kerassurgeon.identify import get_apoz;\n",
    "from kerassurgeon.operations import delete_layer, insert_layer, delete_channels;\n",
    "from kerassurgeon import Surgeon\n",
    "from kerassurgeon import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z10It1ocvDDz"
   },
   "source": [
    "# The Speck cipher and data generation algorithms #\n",
    "Taken from https://github.com/agohr/deep_speck/blob/master/speck.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-As2P_AK_gEW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import urandom\n",
    "\n",
    "def WORD_SIZE():\n",
    "    return(16);\n",
    "\n",
    "def ALPHA():\n",
    "    return(7);\n",
    "\n",
    "def BETA():\n",
    "    return(2);\n",
    "\n",
    "MASK_VAL = 2 ** WORD_SIZE() - 1;\n",
    "\n",
    "def shuffle_together(l):\n",
    "    state = np.random.get_state();\n",
    "    for x in l:\n",
    "        np.random.set_state(state);\n",
    "        np.random.shuffle(x);\n",
    "\n",
    "def rol(x,k):\n",
    "    return(((x << k) & MASK_VAL) | (x >> (WORD_SIZE() - k)));\n",
    "\n",
    "def ror(x,k):\n",
    "    return((x >> k) | ((x << (WORD_SIZE() - k)) & MASK_VAL));\n",
    "\n",
    "def enc_one_round(p, k):\n",
    "    c0, c1 = p[0], p[1];\n",
    "    c0 = ror(c0, ALPHA());\n",
    "    c0 = (c0 + c1) & MASK_VAL;\n",
    "    c0 = c0 ^ k;\n",
    "    c1 = rol(c1, BETA());\n",
    "    c1 = c1 ^ c0;\n",
    "    return(c0,c1);\n",
    "\n",
    "def dec_one_round(c,k):\n",
    "    c0, c1 = c[0], c[1];\n",
    "    c1 = c1 ^ c0;\n",
    "    c1 = ror(c1, BETA());\n",
    "    c0 = c0 ^ k;\n",
    "    c0 = (c0 - c1) & MASK_VAL;\n",
    "    c0 = rol(c0, ALPHA());\n",
    "    return(c0, c1);\n",
    "\n",
    "def expand_key(k, t):\n",
    "    ks = [0 for i in range(t)];\n",
    "    ks[0] = k[len(k)-1];\n",
    "    l = list(reversed(k[:len(k)-1]));\n",
    "    for i in range(t-1):\n",
    "        l[i%3], ks[i+1] = enc_one_round((l[i%3], ks[i]), i);\n",
    "    return(ks);\n",
    "\n",
    "def encrypt(p, ks):\n",
    "    x, y = p[0], p[1];\n",
    "    for k in ks:\n",
    "        x,y = enc_one_round((x,y), k);\n",
    "    return(x, y);\n",
    "\n",
    "def decrypt(c, ks):\n",
    "    x, y = c[0], c[1];\n",
    "    for k in reversed(ks):\n",
    "        x, y = dec_one_round((x,y), k);\n",
    "    return(x,y);\n",
    "\n",
    "def check_testvector():\n",
    "  key = (0x1918,0x1110,0x0908,0x0100)\n",
    "  pt = (0x6574, 0x694c)\n",
    "  ks = expand_key(key, 22)\n",
    "  ct = encrypt(pt, ks)\n",
    "  if (ct == (0xa868, 0x42f2)):\n",
    "    print(\"Testvector verified.\")\n",
    "    return(True);\n",
    "  else:\n",
    "    print(\"Testvector not verified.\")\n",
    "    return(False);\n",
    "\n",
    "#convert_to_binary takes as input an array of ciphertext pairs\n",
    "#where the first row of the array contains the lefthand side of the ciphertexts,\n",
    "#the second row contains the righthand side of the ciphertexts,\n",
    "#the third row contains the lefthand side of the second ciphertexts,\n",
    "#and so on\n",
    "#it returns an array of bit vectors containing the same data\n",
    "def convert_to_binary(arr):\n",
    "  X = np.zeros((4 * WORD_SIZE(),len(arr[0])),dtype=np.uint8);\n",
    "  for i in range(4 * WORD_SIZE()):\n",
    "    index = i // WORD_SIZE();\n",
    "    offset = WORD_SIZE() - (i % WORD_SIZE()) - 1;\n",
    "    X[i] = (arr[index] >> offset) & 1;\n",
    "  X = X.transpose();\n",
    "  return(X);\n",
    "\n",
    "#takes a text file that contains encrypted block0, block1, true diff prob, real or random\n",
    "#data samples are line separated, the above items whitespace-separated\n",
    "#returns train data, ground truth, optimal ddt prediction\n",
    "def readcsv(datei):\n",
    "    data = np.genfromtxt(datei, delimiter=' ', converters={x: lambda s: int(s,16) for x in range(2)});\n",
    "    X0 = [data[i][0] for i in range(len(data))];\n",
    "    X1 = [data[i][1] for i in range(len(data))];\n",
    "    Y = [data[i][3] for i in range(len(data))];\n",
    "    Z = [data[i][2] for i in range(len(data))];\n",
    "    ct0a = [X0[i] >> 16 for i in range(len(data))];\n",
    "    ct1a = [X0[i] & MASK_VAL for i in range(len(data))];\n",
    "    ct0b = [X1[i] >> 16 for i in range(len(data))];\n",
    "    ct1b = [X1[i] & MASK_VAL for i in range(len(data))];\n",
    "    ct0a = np.array(ct0a, dtype=np.uint16); ct1a = np.array(ct1a,dtype=np.uint16);\n",
    "    ct0b = np.array(ct0b, dtype=np.uint16); ct1b = np.array(ct1b, dtype=np.uint16);\n",
    "    \n",
    "    #X = [[X0[i] >> 16, X0[i] & 0xffff, X1[i] >> 16, X1[i] & 0xffff] for i in range(len(data))];\n",
    "    X = convert_to_binary([ct0a, ct1a, ct0b, ct1b]); \n",
    "    Y = np.array(Y, dtype=np.uint8); Z = np.array(Z);\n",
    "    return(X,Y,Z);\n",
    "\n",
    "#baseline training data generator\n",
    "def make_train_data(n, nr, diff=(0x0040,0)):\n",
    "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
    "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
    "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
    "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
    "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
    "  num_rand_samples = np.sum(Y==0);\n",
    "  plain1l[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
    "  plain1r[Y==0] = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
    "  ks = expand_key(keys, nr);\n",
    "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
    "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
    "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
    "  return(X,Y);\n",
    "\n",
    "#real differences data generator\n",
    "def real_differences_data(n, nr, diff=(0x0040,0)):\n",
    "  #generate labels\n",
    "  Y = np.frombuffer(urandom(n), dtype=np.uint8); Y = Y & 1;\n",
    "  #generate keys\n",
    "  keys = np.frombuffer(urandom(8*n),dtype=np.uint16).reshape(4,-1);\n",
    "  #generate plaintexts\n",
    "  plain0l = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
    "  plain0r = np.frombuffer(urandom(2*n),dtype=np.uint16);\n",
    "  #apply input difference\n",
    "  plain1l = plain0l ^ diff[0]; plain1r = plain0r ^ diff[1];\n",
    "  num_rand_samples = np.sum(Y==0);\n",
    "  #expand keys and encrypt\n",
    "  ks = expand_key(keys, nr);\n",
    "  ctdata0l, ctdata0r = encrypt((plain0l, plain0r), ks);\n",
    "  ctdata1l, ctdata1r = encrypt((plain1l, plain1r), ks);\n",
    "  #generate blinding values\n",
    "  k0 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
    "  k1 = np.frombuffer(urandom(2*num_rand_samples),dtype=np.uint16);\n",
    "  #apply blinding to the samples labelled as random\n",
    "  ctdata0l[Y==0] = ctdata0l[Y==0] ^ k0; ctdata0r[Y==0] = ctdata0r[Y==0] ^ k1;\n",
    "  ctdata1l[Y==0] = ctdata1l[Y==0] ^ k0; ctdata1r[Y==0] = ctdata1r[Y==0] ^ k1;\n",
    "  #convert to input data for neural networks\n",
    "  X = convert_to_binary([ctdata0l, ctdata0r, ctdata1l, ctdata1r]);\n",
    "  return(X,Y);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPBJBk7qvTv5"
   },
   "source": [
    "# Evaluate the results #\n",
    "Taken from https://github.com/agohr/deep_speck/blob/master/eval.py and slightly adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JUEUGNtKmM_"
   },
   "outputs": [],
   "source": [
    "def evaluate(net,X,Y):\n",
    "    \n",
    "    Z = net.predict(X,batch_size=10000).flatten();\n",
    "    Zbin = (Z > 0.5);\n",
    "    \n",
    "    #Compute the acc, tpr, tnr\n",
    "    n = len(Z); \n",
    "    n0 = np.sum(Y==0); \n",
    "    n1 = np.sum(Y==1);\n",
    "    \n",
    "    acc = np.sum(Zbin == Y) / n;\n",
    "    tpr = np.sum(Zbin[Y==1]) / n1;\n",
    "    tnr = np.sum(Zbin[Y==0] == 0) / n0;\n",
    "    \n",
    "    return(acc, tpr, tnr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-4h79SxwRNk"
   },
   "source": [
    "# Conduct multiple evaluations #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZvr03IBwQTo"
   },
   "outputs": [],
   "source": [
    "def multiple_evaluations(model, repetitions, num_rounds):\n",
    " \n",
    "  #The accs, tprs, tnrs for all evaluation repetition\n",
    "  accs = [];\n",
    "  tprs = [];\n",
    "  tnrs = [];\n",
    "    \n",
    "  #Evaluate multiple times and average the results \n",
    "  for i in range(0, repetitions):\n",
    "    X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
    "\n",
    "    (acc, tpr, tnr) = evaluate(model, X_eval, Y_eval);\n",
    "    accs.append(acc);\n",
    "    tprs.append(tpr);\n",
    "    tnrs.append(tnr);\n",
    "\n",
    "  print(\"Acc: \" + str(np.mean(accs)) + str(\" +- \") + str(np.std(accs)) + str(\"\\t\") + \n",
    "        \"Tpr:\" + str(np.mean(tprs)) + str(\" +- \") + str(np.std(tprs)) + str(\"\\t\") +\n",
    "        \"Tnr:\" + str(np.mean(tnrs)) + str(\" +- \") + str(np.std(tnrs)) + str(\"\\t\"));\n",
    "        \n",
    "  return(np.mean(accs), np.mean(tprs), np.mean(tnrs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ4vO-xzvHo6"
   },
   "source": [
    "# The depth-1/10 distinguisher implementation #\n",
    "Taken from https://github.com/agohr/deep_speck/blob/master/train_nets.py and slightly adapted for running multiple trials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm_fGK4vQ8Y1"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Conv1D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation, MaxPooling1D, Concatenate,Dropout, AveragePooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import l2, l1, l1_l2\n",
    "\n",
    "\n",
    "def cyclic_lr(num_epochs, high_lr, low_lr):\n",
    "  res = lambda i: low_lr + ((num_epochs-1) - i % num_epochs)/(num_epochs-1) * (high_lr - low_lr);\n",
    "  return(res);\n",
    "\n",
    "#Batch size\n",
    "bs = 5000;\n",
    "\n",
    "\n",
    "def make_resnet(num_blocks=2, num_filters=32, num_outputs=1, d1=64, d2=64, word_size=16, ks=3,depth=5, reg_param=0.0001, final_activation='sigmoid'):\n",
    "  \n",
    "  #Input and preprocessing layers\n",
    "  inp = Input(shape=(num_blocks * word_size * 2,));\n",
    "  rs = Reshape((2 * num_blocks, word_size))(inp);\n",
    "  perm = Permute((2,1))(rs);\n",
    "    \n",
    "  #Block 1\n",
    "  conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm);\n",
    "  conv0 = BatchNormalization()(conv0);\n",
    "  conv0 = Activation('relu')(conv0);\n",
    "    \n",
    "  #Blocks 2-i - residual blocks\n",
    "  shortcut = conv0;\n",
    "  for i in range(depth):\n",
    "    conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut);\n",
    "    conv1 = BatchNormalization()(conv1);\n",
    "    conv1 = Activation('relu')(conv1);\n",
    "    \n",
    "    conv2 = Conv1D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1);\n",
    "    conv2 = BatchNormalization()(conv2);\n",
    "    conv2 = Activation('relu')(conv2);\n",
    "    shortcut = Add()([shortcut, conv2]);\n",
    "    \n",
    "  #Block 3\n",
    "  flat1 = Flatten()(shortcut);\n",
    "    \n",
    "  dense1 = Dense(d1,kernel_regularizer=l2(reg_param))(flat1);\n",
    "  dense1 = BatchNormalization()(dense1);\n",
    "  dense1 = Activation('relu')(dense1);\n",
    "\n",
    "  dense2 = Dense(d2, kernel_regularizer=l2(reg_param))(dense1);\n",
    "  dense2 = BatchNormalization()(dense2);\n",
    "  dense2 = Activation('relu')(dense2);\n",
    "    \n",
    "  out = Dense(num_outputs, activation=final_activation, kernel_regularizer=l2(reg_param))(dense2);\n",
    "\n",
    "  model = Model(inputs=inp, outputs=out);\n",
    "\n",
    "  return model;\n",
    "\n",
    "\n",
    "\n",
    "def model_builder(depth):\n",
    "\n",
    "  model = make_resnet(depth=depth);\n",
    "\n",
    "  model.compile(\n",
    "          optimizer='adam',\n",
    "          loss='binary_crossentropy',\n",
    "          metrics=['acc']);\n",
    "    \n",
    "  return model;\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def train_speck_distinguisher(model, num_epochs, num_rounds, X_train, Y_train, X_eval, Y_eval):\n",
    "    \n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience= 3, restore_best_weights= True);\n",
    "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001));\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size= bs, epochs= num_epochs, validation_data= (X_eval, Y_eval), callbacks=[lr, stop_early,])\n",
    "\n",
    "    return model;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQjZdkFnwWzr"
   },
   "source": [
    "# Obtain filter/neuron indexes with the Average Percentage Of activations equal to Zero greater or equal to *percentage* #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHWi05UjCf1u"
   },
   "outputs": [],
   "source": [
    "def get_indexes_with_apoz_gt(apoz_values, percentage):\n",
    "  \n",
    "  indexes =[];\n",
    "\n",
    "  for i in range(0,len(apoz_values)):\n",
    "    if apoz_values[i] >= percentage:\n",
    "      indexes.append(i);\n",
    "    \n",
    "  #These indexes are used later to know which filter/neuron to prune\n",
    "  return indexes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tAdnjLOxEXm"
   },
   "source": [
    "# Repeat: Prune the model of # filters/neurons with an APoZ value >= *percentage*, train it, and evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoQvbs1tEHiv"
   },
   "outputs": [],
   "source": [
    "def repeat_experiment_for_an_apoz_percentage(num_rounds, depth, num_experiment_trials, prune_above_apoz_percentage):\n",
    "\n",
    "        #Store the accs, tprs, tnrs for all exeperiment repetitions for a speciffic APoZ cutoff (prune_above_apoz_percentage)\n",
    "        accs =[];\n",
    "        tprs =[];\n",
    "        tnrs =[];\n",
    "        \n",
    "        #Store how many filters/neurons were pruned at each layer per experiment\n",
    "        pruned_count = [[],[],[],[],[]];\n",
    "       \n",
    "        #Repeat the experiment multiple times \n",
    "        for trial in range(0, num_experiment_trials):\n",
    "            \n",
    "            #Train the depth-1 distinguisher\n",
    "            X_train, Y_train = make_train_data(10**7,num_rounds);\n",
    "            X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
    "            initial_model = model_builder(depth=1);\n",
    "            trained_model= train_speck_distinguisher(initial_model, 30, num_rounds, X_train, Y_train, X_eval, Y_eval);\n",
    "\n",
    "            #Generate data for computing the APoZ values\n",
    "            X, _ = make_train_data(10**6, num_rounds);\n",
    "\n",
    "            #Store the names of the layer and its activation layer\n",
    "            activation_layers =[];\n",
    "            conv_or_dense_layers=[];\n",
    "            \n",
    "            for layer in trained_model.layers:\n",
    "                \n",
    "                if ('conv' in layer.name or 'dense' in layer.name):\n",
    "                  conv_or_dense_layers.append(layer);\n",
    "                \n",
    "                if('activation' in  layer.name):\n",
    "                  activation_layers.append(layer);\n",
    "\n",
    "            #Instantiate the surgeon\n",
    "            surgeon = Surgeon(trained_model);\n",
    "            \n",
    "\n",
    "            for i in range(0, len(activation_layers)):\n",
    "              \n",
    "              #Get the APoZ values for a speciffic layer\n",
    "              act = get_apoz(trained_model, activation_layers[i], X);\n",
    "              #Get the indexess of the filters/neurons to prune from the layer\n",
    "              prune_indexes=get_indexes_with_apoz_gt(act, prune_above_apoz_percentage); \n",
    "              #Add job to prune the layer later \n",
    "              surgeon.add_job('delete_channels', conv_or_dense_layers[i], channels=prune_indexes);\n",
    "              #Store how much was pruned\n",
    "              pruned_count[i].append(len(prune_indexes)); \n",
    "\n",
    "              \n",
    "\n",
    "            #Prune the model\n",
    "            pruned_model =surgeon.operate();\n",
    "            pruned_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc']);\n",
    "\n",
    "            #Trained the pruned model\n",
    "            X_train, Y_train = make_train_data(10**7,num_rounds);\n",
    "            X_eval, Y_eval = make_train_data(10**6, num_rounds);\n",
    "            trained_pruned_model = train_speck_distinguisher(pruned_model, 30, num_rounds, X_train, Y_train, X_eval, Y_eval);\n",
    "\n",
    "            #Evaluate the pruned model\n",
    "            (acc, tpr, tnr) = multiple_evaluations(trained_pruned_model,5,num_rounds);\n",
    "            accs.append(acc);\n",
    "            tprs.append(tpr);\n",
    "            tnrs.append(tnr);\n",
    "\n",
    "        print(\"Rounds: \"+str(num_rounds) +\" \"+ \"APOZ: \"+str(prune_above_apoz_percentage));\n",
    "        print(\"Average accuracy: \"+str(np.mean(accs)) +\" +/- \"+ str(np.std(accs)));\n",
    "        print(\"Average TPR: \"+str(np.mean(tprs)) +\" +/- \"+ str(np.std(tprs)));\n",
    "        print(\"Average TNR: \"+str(np.mean(tnrs)) +\" +/- \"+ str(np.std(tnrs)));\n",
    "\n",
    "        print(\"Average pruned filters at Conv1: \"+ str(np.mean(pruned_count[0])));\n",
    "        print(\"Average pruned filters at Conv2: \"+ str(np.mean(pruned_count[1])));\n",
    "        print(\"Average pruned filters at Conv3: \"+ str(np.mean(pruned_count[2])));\n",
    "        print(\"Average pruned neurons at Dense1: \"+ str(np.mean(pruned_count[3])));\n",
    "        print(\"Average pruned neurons at Dense2: \"+ str(np.mean(pruned_count[4])));\n",
    "\n",
    "        #Return the above-printed values\n",
    "        return (np.mean(accs), np.mean(tprs), np.mean(tnrs), accs, tprs, tnrs, pruned_count[0], pruned_count[1], pruned_count[2], pruned_count[3], pruned_count[4]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the experiment #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dB-1WAe9QMo1"
   },
   "outputs": [],
   "source": [
    "# num_rounds = 5, 6, 7, 8\n",
    "# prune_above_apoz_percentage = 1, 0.9, 0.8, 0.7\n",
    "\n",
    "(acc_avg, tpr_avg, tnr_avg, accs, tprs, tnrs, pc1, pc2, pc3, pd1, pd2) = repeat_experiment_for_an_apoz_percentage(num_rounds =  5, depth = 1, num_experiment_trials = 5, prune_above_apoz_percentage = 1);"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TheSpeckAttack_Prune_with_Surgeon_based_on_APoZ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
